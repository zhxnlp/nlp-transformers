## 复习笔记2——线性回归、决策树、聚类
@[toc]
## 一、.线性回归

	fit_intercept : 布尔型参数，表示是否计算该模型截距。可选参数。
	normalize : 布尔型参数，若为True，则X在回归前进行归一化。可选参数。默认值为False。
	copy_X : 布尔型参数，若为True，则X将被复制；否则将被覆盖。 可选参数。默认值为True。
	n_jobs : 整型参数，表示用于计算的作业数量；若为-1，则用所有的CPU。可选参数。默认为1
	positive=False#当设置为'True'时，强制系数为正。这选项仅支持密集阵列。
	
	rint(model.coef_)#打印线性方程中的w
	print(model.intercept_)#打印w0 就是线性方程中的截距b

#### 3.sklearn.metrics 
模块包括评分函数、性能指标和成对度量和距离计算。
F1-score:    2*(P*R)/(P+R)。[参考《sklearn中 F1-micro 与 F1-macro区别和计算原理》](https://www.cnblogs.com/techengin/p/8962024.html)
导入：from sklearn import metrics
分类指标

	accuracy_score(y_true, y_pre)#精度
	
	log_loss(y_true, y_pred, eps=1e-15, normalize=True, sample_weight=None, labels=None)交叉熵损失函数
	
	auc(x, y, reorder=False)
	ROC曲线下的面积;较大的AUC代表了较好的performance。
	AUC：roc_auc_score(y_true, y_score, average=‘macro’, sample_weight=None)
	
	f1_score(y_true, y_pred, labels=None, pos_label=1, average=‘binary’, sample_weight=None) F1值
		
	precision_score(y_true, y_pred, labels=None, pos_label=1, average=‘binary’,) 查准率
	
	recall_score(y_true, y_pred, labels=None, pos_label=1, average=‘binary’, sample_weight=None) 查全率
	roc_curve(y_true, y_score, pos_label=None, sample_weight=None, drop_intermediate=True)
	计算ROC曲线的横纵坐标值，TPR，FPR
	TPR = TP/(TP+FN) = recall(真正例率，敏感度)
	FPR = FP/(FP+TN)(假正例率，1-特异性)
	
	classification_report(y_true, y_pred)#分类结果分析汇总

f1_score中关于参数average的用法描述:
'micro':Calculate metrics globally by counting the total true positives, false negatives and false positives.

'micro':通过先计算总体的TP，FN和FP的数量，再计算F1

'macro':Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account.

'macro':分布计算每个类别的F1，然后做平均（各类别F1的权重相同）
回归指标

	explained_variance_score(y_true, y_pred, sample_weight=None, multioutput=‘uniform_average’)
	回归方差(反应自变量与因变量之间的相关程度)
	mean_absolute_error(y_true, y_pred, sample_weight=None, multioutput=‘uniform_average’)
	平均绝对误差MAE
	
	mean_squared_error(y_true, y_pred, sample_weight=None, multioutput=‘uniform_average’) #均方差MSE
	
	median_absolute_error(y_true, y_pred)
	中值绝对误差
	r2_score(y_true, y_pred, sample_weight=None, multioutput=‘uniform_average’) #R平方值
	
#### 4.PolynomialFeatures构建特征
https://blog.csdn.net/tiange_xiao/article/details/79755793
使用sklearn.preprocessing.PolynomialFeatures来进行特征的构造。
degree：控制多项式的度
interaction_only： 默认为False，如果指定为True，那么就不会有特征自己和自己结合的项，上面的二次项中没有a^2和b^2。
include_bias：默认为True。如果为True的话，那么就会有上面的 1那一项。
#### 5.机器学习中的random_state参数
原文链接：https://blog.csdn.net/ytomc/article/details/113437926

	1、在构建模型时：
		forest = RandomForestClassifier(n_estimators=100, random_state=0)
		forest.fit(X_train, y_train)
	2、在生成数据集时：
		X, y = make_moons(n_samples=100, noise=0.25, random_state=3)
	
	3、在拆分数据集为训练集、测试集时：
		X_train, X_test, y_train, y_test = train_test_split(
	cancer.data, cancer.target, stratify=cancer.target, random_state=42)
		参数test_size：如果是浮点数，在0-1之间，表示test set的样本占比；如果是整数的话就表示test set样本数量。

例如1中，每次构建的模型是不同的。
例如2中，每次生成的数据集是不同的。
例如3中，每次拆分出的训练集、测试集是不同的。

#### 5.Solver lbfgs supports only “l2” or “none” penalties, got l1 penalty.解决办法
在用以下代码建立逻辑回归模型的时候
lr = LogisticRegression(C = c_param, penalty = ‘l1’)，正则化惩罚选择’L1’报错。
LogisticRegression的参数如下：

	LogisticRegression(C=0.01, class_weight=None, dual=False, fit_intercept=True,intercept_scaling=1, l1_ratio=None, max_iter=100,multi_class='auto', n_jobs=None, penalty='l1',random_state=None, solver='lbfgs', tol=0.0001, verbose=0,warm_start=False)

我们看solver参数，这个参数定义的是分类器，‘newton-cg’，‘sag’和‘lbfgs’等solvers仅支持‘L2’regularization，‘liblinear’ solver同时支持‘L1’、‘L2’regularization，若dual=Ture，则仅支持L2 penalty。
决定惩罚项选择的有2个参数：dual和solver，如果要选L1范数，dual必须是False，solver必须是liblinear
因此，我们只需将solver='liblinear’参数添加进去即可

	lr = LogisticRegression(C = c_param, penalty = ‘l1’,solver=‘liblinear’)

原文链接：https://blog.csdn.net/kakak_/article/details/104923634

## 二、决策树和随机森林
随机森林算法详解：https://zhuanlan.zhihu.com/p/139510947
&#8195;&#8195;GBDT、XGBOOST、LGBM都是以决策树为积木搭建出来的。
&#8195;&#8195;随机森林就是决策树们基于bagging集成学习思想搭建起来的。算法实现思路非常简单，只需要记住一句口诀：抽等量样本，选几个特征，构建多棵树。
#### 2.1随机森林的随机性：
&#8195;&#8195;数据集的随机选取、每棵树所使用特征的随机选取。以上两个3.1随机性使得随机森林中的决策树都能够彼此不同，提升系统的多样性，从而提升分类性能。
1）抽等量样本
&#8195;&#8195;抽样方式一般是有放回的抽样，也就是说，在训练某棵树的时候，这一次被抽到的样本会被放回数据集中，下一次还可能被抽到，因此，原训练集中有的样本会多次被抽出用来训练，而有的样本可能不会被使用到。
但是不用担心有的样本没有用到，只要训练的树的棵数足够多，大多数训练样本总会被取到的。有极少量的样本成为漏网之鱼也不用担心，后边我们会筛选他们出来用来测试模型。

2）选几个特征（“max_features”）
&#8195;&#8195;在训练某棵树的时候，会随机选择一部分特征用来训练。这样做的目的就是让不同的树重点关注不同的特征。

3）构建多棵树（“n_estimators”）
&#8195;&#8195;最终的结果由每棵决策树综合给出：如果是分类问题，所有树投票决定；如果是回归问题，各个树得到的结果加权平均。（每个树的结果是叶节点的均值，预测房价，就是样本输入模型分到某个节点，这个叶节点所以=有房子价格的均值。一棵树m个叶节点，就只有m个输出）所以随机森林做回归比较少。
#### 2.2优缺点：
1）实现简单，泛化能力强，可以并行实现，因为训练时树与树之间是相互独立的；
2）相比单一决策树，能学习到特征之间的相互影响，且不容易过拟合；
3）能直接特征很多的高维数据，因为在训练过程中依旧会从这些特征中随机选取部分特征用来训练；
4）相比SVM，不是很怕特征缺失，因为待选特征也是随机选取；
5）训练完成后可以给出特征重要性。当然，这个优点主要来源于决策树。因为决策树在训练过程中会计算熵或者是基尼系数，越往树的根部，特征越重要。
2、缺点
1）在噪声过大的分类和处理回归问题时还是容易过拟合；
2）相比于单一决策树，它的随机性让我们难以对模型进行解释。
#### 2.3调参
1、用于调参的参数：
max_features（最大特征数）: 这个参数用来训练每棵树时需要考虑的最大特征个数，超过限制个数的特征都会被舍弃，默认为auto。可填入的值有：int值，float（特征总数目的百分比），“auto”/“sqrt”（总特征个数开平方取整）,“log2”（总特征个数取对数取整）。默认值为总特征个数开平方取整。值得一提的是，这个参数在决策树中也有但是不重要，因为其默认为None，即有多少特征用多少特征。为什么要设置这样一个参数呢？原因如下：考虑到训练模型会产生多棵树，如果在训练每棵树的时候都用到所有特征，以来导致运算量加大，二来每棵树训练出来也可能千篇一律，没有太多侧重，所以，设置这个参数，使训练每棵树的时候只随机用到部分特征，在减少整体运算的同时还可以让每棵树更注重自己选到的特征。

n_estimators：随机森林生成树的个数，默认为100。

2、控制样本抽样参数：
bootstrap：每次构建树是不是采用有放回样本的方式(bootstrap samples)抽取数据集。可选参数：True和False，默认为True。
oob_score：是否使用袋外数据来评估模型，默认为False。
boostrap和 oob_score两个参数一般要配合使用。如果boostrap是False，那么每次训练时都用整个数据集训练，如果boostrap是True，那么就会产生袋外数据。
选择criterion参数（决策树划分标准）
和决策树一样，这个参数只有两个参数 'entropy'（熵） 和 'gini'（基尼系数）可选，默认为gini

有放回抽样也会有自己的问题。由于是有放回，一些样本可能在同一个自助集中出现多次，而其他一些却可能被忽略，一般来说，每一次抽样，某个样本被抽到的概率是 1/n ，所以不被抽到的概率就是 1-1/n ,所以n个样本都不被抽到的概率就是： 
用洛必达法则化简，可以得到这个概率收敛于(1/e)，约等于0.37。
因此，如果数据量足够大的时候，会有约37%的训练数据被浪费掉，没有参与建模，这些数据被称为袋外数据(out of bag data，简写为oob)。
为了这些数据不被浪费，我们也可以把他们用来作为集成算法的测试集。也就是说，在使用随机森林时，我们可以不划分测试集和训练集，只需要用袋外数据来测试我们的模型即可。

### 三、聚类
见笔记
聚类是指试图将相似的数据点分组到人工确定的组或簇中
1. 聚类的基本思想：对于给定的M个样本的数据集，给定聚类（簇）的个数K（K<M），初始化每个样本所属的类别，再根据一定的规则不断地迭代并重新划分数据集的类别（改变样本与簇的类别关系），使得每一次的划分都比上一次的划分要好。
2. 聚类算法有很多种，主要分为划分聚类（KMeans）、密度聚类（DBSCAN）和谱聚类等三种聚类。
#### 3.2 kmeans
1. KMeans算法的思想：对于给定的M个样本的数据集（无标签），给定聚类（簇）的个数K（K<M），初始化每个样本所属的类别，再根据距离的不同，将每个样本分配到距离最近的中心点的簇中，然后再对迭代完成的每个簇更新中心点位置（改变样本与簇的类别关系），直到达到终止条件为止。
2. KMeans算法的终止条件：1）迭代次数    2）簇中心点变化率    3）最小平方误差值
3. KMeans算法的优点：理解简单容易，凸聚类的效果不错，效率高；对于服从高斯分布的数据集效果相当好。

 4. KMeans算法的缺点：
- 对初始点敏感（局部最优）
- 被异常点影响（异常点影响聚类中心值，聚类边界点有可能被分到另一类，异常点很难清洗）
- 某些场景中心点缺乏物理意义（例如分类性别以0和1表示，中心点可能是小数）
- 数值问题（例如以人的身高体重做聚类，不同单位数值差别很大。例如身高以m为单位基本差别很小，以g为单位差别很大，此时基本由体重主导了，所以每一维要做归一化。可以让方差为1或者都除以最大值）
- K值不好选择
K-means本身有很多问题，因为没有数据标注，是没有办法的办法


伪代码：

初始化K个中心点（一般随机选择）：

    当中心点变化大于给定值或者小于迭代次数时：
        对于数据集中的每个点：
            对于每个中心点：
                计算数据点到中心点的距离
            将数据点分配到距离最近的簇中
        对于每个簇，计算簇中所有数据点的平均值，更新簇中心点的位置
    返回最终的簇中心点和对应的簇

####3.3 DBSCAN聚类算法
DBSCAN的算法思想：用一个点附近的邻域内的数据点的个数来衡量该点所在空间的密度。






