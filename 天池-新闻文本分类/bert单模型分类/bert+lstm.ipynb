{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"bert+lstm.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"interpreter":{"hash":"3bfce0b4c492a35815b5705a19fe374a7eea0baaa08b34d90450caf1fe9ce20b"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"9c18d2c69bef4e7eb6c766b4d1693456":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_cf12a97113a0403482626ad12aa18012","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_36114ee055284b22b9ea2abf7434ed59","IPY_MODEL_c750e6ba23164ecdab7dd8986bfd915e","IPY_MODEL_be2f1f9d8c4d4eff9c8a6f1376467679"]}},"cf12a97113a0403482626ad12aa18012":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"36114ee055284b22b9ea2abf7434ed59":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_1f9a1277dd9e4516866363844baabaf9","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_321333dd5e934d48ab780a6d2b9b382a"}},"c750e6ba23164ecdab7dd8986bfd915e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_4bfc8e01c45f437898f57f8b02fa5ea8","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"","max":3125,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":3125,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_628cae82b7704dd7a40a8ae1ae01f19e"}},"be2f1f9d8c4d4eff9c8a6f1376467679":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_57096380baeb42e7bd2a44a3d6b193cf","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 3125/3125 [51:05&lt;00:00,  1.02it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b1aee913ba274587962787255fb5ed2f"}},"1f9a1277dd9e4516866363844baabaf9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"321333dd5e934d48ab780a6d2b9b382a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4bfc8e01c45f437898f57f8b02fa5ea8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"628cae82b7704dd7a40a8ae1ae01f19e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"57096380baeb42e7bd2a44a3d6b193cf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"b1aee913ba274587962787255fb5ed2f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"jZ3H30DSeoXw"},"source":[""]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s7Fwcp5CLXtg","executionInfo":{"status":"ok","timestamp":1636628646860,"user_tz":-480,"elapsed":21531,"user":{"displayName":"张hongxu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01344108933923387301"}},"outputId":"3ab1233c-bcf6-469b-eb55-961502bccf0a"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"id":"SjDdD4vHN1D6"},"source":["import os\n","os.chdir('/content/drive/MyDrive/transformers/天池-入门NLP - 新闻文本分类')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"laWQqNegpwhR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636628663164,"user_tz":-480,"elapsed":15011,"user":{"displayName":"张hongxu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01344108933923387301"}},"outputId":"b8a2601e-2028-4d18-ac5a-6dfb7a1bcdcd"},"source":["#安装\n","!pip install transformers datasets"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.12.3-py3-none-any.whl (3.1 MB)\n","\u001b[K     |████████████████████████████████| 3.1 MB 2.0 MB/s \n","\u001b[?25hCollecting datasets\n","  Downloading datasets-1.15.1-py3-none-any.whl (290 kB)\n","\u001b[K     |████████████████████████████████| 290 kB 72.4 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.2)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.3.2)\n","Collecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.1.2-py3-none-any.whl (59 kB)\n","\u001b[K     |████████████████████████████████| 59 kB 4.2 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 67.0 MB/s \n","\u001b[?25hCollecting sacremoses\n","  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 63.9 MB/s \n","\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 23.6 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n","Requirement already satisfied: pyparsing<3,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n","Collecting fsspec[http]>=2021.05.0\n","  Downloading fsspec-2021.11.0-py3-none-any.whl (132 kB)\n","\u001b[K     |████████████████████████████████| 132 kB 50.1 MB/s \n","\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n","Collecting aiohttp\n","  Downloading aiohttp-3.8.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n","\u001b[K     |████████████████████████████████| 1.1 MB 61.9 MB/s \n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n","Collecting xxhash\n","  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n","\u001b[K     |████████████████████████████████| 243 kB 57.0 MB/s \n","\u001b[?25hRequirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Collecting asynctest==0.13.0\n","  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n","Collecting multidict<7.0,>=4.5\n","  Downloading multidict-5.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (160 kB)\n","\u001b[K     |████████████████████████████████| 160 kB 67.8 MB/s \n","\u001b[?25hCollecting yarl<2.0,>=1.0\n","  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n","\u001b[K     |████████████████████████████████| 271 kB 50.3 MB/s \n","\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.2.0)\n","Collecting frozenlist>=1.1.1\n","  Downloading frozenlist-1.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (192 kB)\n","\u001b[K     |████████████████████████████████| 192 kB 70.4 MB/s \n","\u001b[?25hRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.7)\n","Collecting aiosignal>=1.1.2\n","  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n","Collecting async-timeout<5.0,>=4.0.0a3\n","  Downloading async_timeout-4.0.1-py3-none-any.whl (5.7 kB)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Installing collected packages: multidict, frozenlist, yarl, asynctest, async-timeout, aiosignal, pyyaml, fsspec, aiohttp, xxhash, tokenizers, sacremoses, huggingface-hub, transformers, datasets\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed aiohttp-3.8.0 aiosignal-1.2.0 async-timeout-4.0.1 asynctest-0.13.0 datasets-1.15.1 frozenlist-1.2.0 fsspec-2021.11.0 huggingface-hub-0.1.2 multidict-5.2.0 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.12.3 xxhash-2.0.2 yarl-1.7.2\n"]}]},{"cell_type":"code","metadata":{"id":"tc_O83qsY8yf"},"source":["#定义全局超参数\n","batch_size=64\n","lr=1e-5\n","dropout=0.15\n","epoch=1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TnHEQBaqTLIc"},"source":["#将3750/648/900改成标点符号，删除原text列，新增列重名为text列\n","import re\n","def replacepunc(x):\n","  x=re.sub('3750',\",\",x)\n","  x=re.sub('900',\".\",x)\n","  x=re.sub('648',\"!\",x)\n","  return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JYx0116UxIgj"},"source":["#保存分词器模型并重新加载\n","#tokenizer.save(\"tokenizers.json\")\n","\n","from transformers import PreTrainedTokenizerFast\n","fast_tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"tokenizers.json\",\n","   model_max_length=512,mask_token='[MASK]',pad_token='[PAD]',unk_token='[UNK]',\n","   cls_token='[CLS]',sep_token='[SEP]',padding_side='right',\n","   return_special_tokens_mask=True)\n","#PreTrainedTokenizerFast中一定要设置mask_token，pad_token等，不然报错"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4hXP70j0CeHk"},"source":["#加载数据\n","\n","from datasets import Dataset\n","import pandas as pd\n","train_df=pd.read_csv('./train_set.csv',sep='\\t',nrows=100).sample(frac=1)\n","\n","#将训练数据中三个token换成标点\n","train_df['texts']=train_df['text'].map(lambda x:replacepunc(x))\n","\n","#准备将text文本首尾截断，各取255tokens\n","def slipt2(x):\n","  ls=x.split(' ')\n","  le=len(ls)\n","  if le<511:\n","    return x\n","  else:\n","    return ' '.join(ls[:255]+ls[-255:]) \n","\n","train_df['summary']=train_df['texts'].apply(lambda x:slipt2(x))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qkjQJhWA41yE"},"source":["\"\"\"数据量太大的话考虑block_size\n","block_size = 128\n","def group_texts(examples):\n","  total_length=len(examples[list(examples.keys())[0]])\n","\"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UEjmwoMr6uJE"},"source":["参考HF的notebook[《custom_datasets》](https://github.com/huggingface/notebooks/blob/master/transformers_doc/custom_datasets.ipynb)"]},{"cell_type":"code","metadata":{"id":"QkmlAhfV47JV"},"source":["#最后一个epoch读取全部数据\n","batch_size=8\n","#数据预处理\n","train_encoding=fast_tokenizer(list(train_df['summary']),truncation=True,padding=True,return_tensors='pt')\n","\n","import torch\n","# 数据集读取\n","class XFeiDataset(Dataset):\n","  def __init__(self,encodings,labels):\n","    self.encodings=encodings\n","    self.labels=labels\n","  \n","  # 读取单个样本\n","  def __getitem__(self,idx):\n","    item={key:torch.tensor(val[idx]) for key,val in self.encodings.items()}\n","    item['labels']=torch.tensor(int(self.labels[idx]))\n","    return item\n","  \n","  def __len__(self):\n","    return len(self.labels)\n","\n","train_dataset=XFeiDataset(train_encoding,list(train_df['label']))\n","\n","# 单个读取到批量读取\n","from torch.utils.data import Dataset, DataLoader,TensorDataset\n","\n","train_loader=DataLoader(train_dataset,batch_size=batch_size,shuffle=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FoUGohsYnegX","executionInfo":{"status":"ok","timestamp":1636629017524,"user_tz":-480,"elapsed":7,"user":{"displayName":"张hongxu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01344108933923387301"}},"outputId":"92bccfdc-9c45-4334-a83f-2f03436e82ae"},"source":["for x in train_loader:\n","    print(x)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'input_ids': tensor([[   1, 1070, 1576,  ...,    3,    3,    3],\n","        [   1,  455,  617,  ..., 1059,  562,    2],\n","        [   1,  551, 1456,  ...,    7,  159,    2],\n","        [   1,  100,  437,  ...,    3,    3,    3]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n","        [0, 0, 0,  ..., 0, 0, 0],\n","        [0, 0, 0,  ..., 0, 0, 0],\n","        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 1, 1, 1],\n","        [1, 1, 1,  ..., 1, 1, 1],\n","        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([ 3,  3,  2, 11])}\n","{'input_ids': tensor([[   1,  728, 1350,  ...,  403,  266,    2],\n","        [   1,  262,  872,  ...,    3,    3,    3],\n","        [   1, 1089,  343,  ..., 1378, 1139,    2],\n","        [   1,  210, 1546,  ...,  678,  666,    2]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n","        [0, 0, 0,  ..., 0, 0, 0],\n","        [0, 0, 0,  ..., 0, 0, 0],\n","        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 1, 1, 1],\n","        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([ 2,  3,  9, 10])}\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  from ipykernel import kernelapp as app\n"]}]},{"cell_type":"code","metadata":{"id":"AbseNgWPZQ40"},"source":["#少量数据读取没问题，全量数据会爆,选用更高配置的colab\n","from sklearn.model_selection import train_test_split\n","\n","train_data,val_data,train_label,val_label=train_test_split(\n","    train_df['summary'].iloc[:], \n","    train_df['label'].iloc[:],\n","    test_size=0.1, \n","    stratify=train_df['label'].iloc[:])\n","\n","#数据预处理\n","train_encoding=fast_tokenizer(list(train_data),truncation=True,padding=True,return_tensors='pt')\n","val_encoding=fast_tokenizer(list(val_data),truncation=True,padding=True,return_tensors='pt')\n","\n","import torch\n","# 数据集读取\n","class XFeiDataset(Dataset):\n","  def __init__(self,encodings,labels):\n","    self.encodings=encodings\n","    self.labels=labels\n","  \n","  # 读取单个样本\n","  def __getitem__(self,idx):\n","    item={key:torch.tensor(val[idx]) for key,val in self.encodings.items()}\n","    item['labels']=torch.tensor(int(self.labels[idx]))\n","    return item\n","  \n","  def __len__(self):\n","    return len(self.labels)\n","\n","train_dataset=XFeiDataset(train_encoding,list(train_label))\n","val_dataset=XFeiDataset(val_encoding,list(val_label))\n","\n","# 单个读取到批量读取\n","from torch.utils.data import Dataset, DataLoader,TensorDataset\n","\n","\n","train_loader=DataLoader(train_dataset,batch_size=batch_size,shuffle=True)\n","val_loader=DataLoader(val_dataset,batch_size=batch_size,shuffle=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6WEZNlC3mhAk","executionInfo":{"status":"ok","timestamp":1636280548303,"user_tz":-480,"elapsed":383,"user":{"displayName":"张hongxu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01344108933923387301"}},"outputId":"7845e234-380d-42c3-e2bf-066f1e5fe6b6"},"source":["import torch\n","#检查其中一个批次：\n","for batch in train_loader:\n","    break\n","{k: v.shape for k, v in batch.items()}"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  from ipykernel import kernelapp as app\n"]},{"output_type":"execute_result","data":{"text/plain":["{'attention_mask': torch.Size([64, 512]),\n"," 'input_ids': torch.Size([64, 512]),\n"," 'labels': torch.Size([64]),\n"," 'token_type_ids': torch.Size([64, 512])}"]},"metadata":{},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"o8-CUD8YhMh7"},"source":["模型结构参考：[《NER:CLUENER2020/BERT-LSTM-CRF/model.py》](https://github.com/hemingkx/CLUENER2020/blob/main/BERT-LSTM-CRF/model.py)\n","\n","[《bert_bi-lstm代码解读》](https://blog.csdn.net/qq_41821608/article/details/115784406?ops_request_misc=&request_id=&biz_id=102&utm_term=bert+lstm&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-9-115784406.pc_search_all_es&spm=1018.2226.3001.4187)"]},{"cell_type":"code","metadata":{"id":"9sUlOvuz6RV-"},"source":["from transformers import BertModel\n","from torch.nn.utils.rnn import pad_sequence\n","#初始化bert模型\n","from transformers import BertConfig\n","import torch.nn as nn\n","from torch.nn import LSTM \n","from torchcrf import CRF\n","\n","class Bert_LSTM(nn.Module):\n","  def __init__(self):\n","    super(Bert_LSTM,self).__init__()\n","    self.num_labels=14\n","    self.dropout=nn.Dropout(dropout)\n","    self.bert=BertModel.from_pretrained(\"./bert_model/pre_Bert-8-epoch\")#这是之前做mlm任务预训练好的bert模型\n","    for param in self.bert.parameters():\n","      param.requires_grad=True\n","    self.classifier=nn.Linear(1024,14)\n","    \n","    self.bilstm=nn.LSTM(\n","        input_size=512, \n","        hidden_size=1024 // 2,  # 1024\n","        batch_first=True,\n","        num_layers=2,\n","        dropout=0.5,  # 0.5\n","        bidirectional=True)\n","\n","  def forward(self,batch_seqs,batch_seq_masks,batch_seq_segments):\n","\n","    output=self.bert(input_ids=batch_seqs,attention_mask=batch_seq_masks,token_type_ids=batch_seq_segments)\n","    pooler_output=output.pooler_output\n","    last_hidden_state=output.last_hidden_state\n","    #只有这种写法不会报错，如果是sequence_output,pooler_output=self.bert(**kwags)这种，sequence_output会报错str没有xxx属性。\n","    #貌似是bert输出有很多，直接用output.last_hidden_state来调用结果（估计是版本问题，坑），关键是输出要打印出来\n","    if model.train():\n","      drop_hidden_state=self.dropout(last_hidden_state)\n","\n","    #output为输出序列的隐藏层，hn为最后一个时刻的隐藏层，cn为最后一个时刻的隐藏细胞\n","    #双向lstm,将最后一层最后时刻hn两个方向的结果拼接起来（就是最后一层的双向hidden结果）\n","    lstm_output,(hn,cn)=self.bilstm(drop_hidden_state)\n","    hidden_last_out=torch.cat([hn[-2],hn[-1]],dim=-1)\n","    if model.train():\n","      hidden_last_out=self.dropout(hidden_last_out)\n","\n","\n","    # 得到判别值\n","    logits=self.classifier(hidden_last_out)\n","    return logits   "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8tfZRqpsLov2"},"source":["#加载模型\n","model=Bert_LSTM()\n","model.load_state_dict(torch.load(\"best_bert_model_3epoch\"))\n","device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4QaL090XFpre"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ygOSQXHUgM0t"},"source":["优化器写法参考文档[《NER代码：CLUENER2020/BERT-LSTM-CRF/run.py》](https://github.com/hemingkx/CLUENER2020/blob/main/BERT-LSTM-CRF/run.py)\n","\n","以及HF教程的[自定义训练循环](https://blog.csdn.net/qq_56591814/article/details/120147114)"]},{"cell_type":"code","metadata":{"id":"ufIxxAIZxhBI"},"source":["#定义优化器\n","from transformers import AdamW,get_scheduler\n","\n","train_steps_per_epoch=len(train_loader)\n","num_training_steps=train_steps_per_epoch*epoch\n","\n","#定义各模块参数\n","bert_parameters=list(model.bert.named_parameters())\n","lstm_parameters=list(model.bilstm.named_parameters())\n","classifier_parameters=list(model.classifier.named_parameters())\n","no_decay=['bias','LayerNorm.weight']\n","\n","\n","optimizer_grouped_parameters=[\n","    {'params':[p for n,p in bert_parameters if not any(nd in n for nd in no_decay)],\n","      'lr':lr,'weight_decay':0.01},\n","    {'params':[p for n,p in bert_parameters if any(nd in n for nd in no_decay)],\n","      'lr':lr,'weight_decay':0.0},\n","    {'params':[p for n,p in lstm_parameters if not any(nd in n for nd in no_decay)],\n","      'lr':lr*3,'weight_decay':0.01},\n","    {'params':[p for n,p in lstm_parameters if any(nd in n for nd in no_decay)],\n","      'lr':lr*3,'weight_decay': 0.0},\n","    {'params':[p for n,p in classifier_parameters if not any(nd in n for nd in no_decay)],\n","      'lr':lr*3,'weight_decay':0.01},\n","    {'params':[p for n,p in classifier_parameters if any(nd in n for nd in no_decay)],\n","      'lr':lr*3,'weight_decay':0.0}]\n","\n","optimizer=AdamW(optimizer_grouped_parameters,lr=lr,eps=1e-8)\n","\n","lr_scheduler=get_scheduler(\n","    \"linear\",\n","    optimizer=optimizer,\n","    num_warmup_steps=0,\n","    num_training_steps=num_training_steps)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ahf5WsrDCtnt"},"source":["import torch\n","for idx,j in enumerate(train_loader):\n","  print(idx,j)\n","  break"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZiSps30fiDoT"},"source":["训练、验证、预测部分参照：[《9102年，再不会用bert就out了》](https://zhuanlan.zhihu.com/p/112655246?utm_source=wechat_session&utm_medium=social&utm_oi=1400823417357139968&utm_campaign=shareopn)\n","\n","[《10分钟 杀入科大讯飞中文相似度 Top10》](https://mp.weixin.qq.com/s/PDwCO-7TUIZfVkQF8aYHiQ)"]},{"cell_type":"code","metadata":{"id":"d8nsox9XBaSh","colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["9c18d2c69bef4e7eb6c766b4d1693456","cf12a97113a0403482626ad12aa18012","36114ee055284b22b9ea2abf7434ed59","c750e6ba23164ecdab7dd8986bfd915e","be2f1f9d8c4d4eff9c8a6f1376467679","1f9a1277dd9e4516866363844baabaf9","321333dd5e934d48ab780a6d2b9b382a","4bfc8e01c45f437898f57f8b02fa5ea8","628cae82b7704dd7a40a8ae1ae01f19e","57096380baeb42e7bd2a44a3d6b193cf","b1aee913ba274587962787255fb5ed2f"]},"executionInfo":{"status":"ok","timestamp":1636280803430,"user_tz":-480,"elapsed":962,"user":{"displayName":"张hongxu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01344108933923387301"}},"outputId":"d1eb74a7-ff28-4afa-a442-0a3cf21f98cf"},"source":["#编写训练和验证循环\n","import time\n","import numpy as np\n","from sklearn.metrics import f1_score,precision_score,recall_score,accuracy_score\n","#加载进度条\n","from tqdm.auto import tqdm\n","num_training_steps=train_steps_per_epoch*epoch\n","\n","progress_bar=tqdm(range(num_training_steps))\n","\n","def train_and_eval(epoch):\n","  best_acc=0.0\n","  criterion=nn.CrossEntropyLoss()\n","  for i in range(epoch):\n","    \"\"\"训练模型\"\"\"\n","    start=time.time()\n","    model.train()\n","    print(\"***** Running training epoch {} *****\".format(i+1))\n","    train_loss_sum=0.0\n","    for idx,batch in enumerate(train_loader):\n","      input_ids=batch['input_ids'].to(device)\n","      attention_mask=batch['attention_mask'].to(device)\n","      token_type_ids=batch['token_type_ids'].to(device)\n","      labels=batch['labels'].to(device)\n","\n","      #计算输出和loss\n","      logits=model(input_ids,attention_mask,token_type_ids)\n","      loss=criterion(logits,labels)\n","      loss.backward()\n","\n","      optimizer.step()\n","      lr_scheduler.step()\n","      optimizer.zero_grad()  \n","      progress_bar.update(1)\n","\n","      train_loss_sum+=loss.item()\n","      if (idx+1)%(len(train_loader)//5)==0: # 只打印五次结果\n","        print(\"Epoch {:03d} | Step {:04d}/{:04d} | Loss {:.4f} | Time {:.4f} | Learning rate = {} \\n\".format(\n","                  i+1,idx+1,len(train_loader),train_loss_sum/(idx+1),time.time()-start,optimizer.state_dict()['param_groups'][0]['lr']))\n","      \n","      #验证模型\n","    model.eval()\n","    total_eval_accuracy=0\n","    total_eval_loss=0\n","    for batch in val_loader:\n","      with torch.no_grad():#只有这一块是不需要求导的\n","      \n","        input_ids=batch['input_ids'].to(device)\n","        attention_mask=batch['attention_mask'].to(device)\n","        token_type_ids=batch['token_type_ids'].to(device)\n","        labels=batch['labels'].to(device)\n","        logits=model(input_ids,attention_mask,token_type_ids)\n","      \n","      loss=criterion(logits,labels)#计算loss和准确率\n","      total_eval_loss+=loss.item()\n","\n","      logits=logits.clone().detach().cpu().numpy()#detach表示复制且不可求导，原tensor不变，仍可求导\n","      predictions=np.argmax(logits,axis=-1)\n","      label_ids=labels.to('cpu').numpy()\n","      total_eval_accuracy+=accuracy_score(predictions,label_ids)\n","\n","    avg_val_accuracy=total_eval_accuracy/len(val_loader)\n","    if avg_val_accuracy>best_acc:\n","      best_acc=avg_val_accuracy\n","      torch.save(model.state_dict(),\"best_bert_model\")\n","    \n","    print(\"val_accuracy:%.4f\" % (avg_val_accuracy))\n","    print(\"Average val loss: %.4f\"%(total_eval_loss/len(val_loader)))\n","    print(\"time costed={}s \\n\".format(round(time.time()-start,5)))\n","    print(\"-------------------------------\")"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9c18d2c69bef4e7eb6c766b4d1693456","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/3125 [00:00<?, ?it/s]"]},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"n5z-8IcAIqHW"},"source":[""]},{"cell_type":"code","metadata":{"id":"VHbmCPw7FdGi","colab":{"base_uri":"https://localhost:8080/","height":255},"executionInfo":{"status":"ok","timestamp":1636283868830,"user_tz":-480,"elapsed":730187,"user":{"displayName":"张hongxu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01344108933923387301"}},"outputId":"4dbea930-7820-4fb4-c9b1-e56b65cf3667"},"source":["train_and_eval(epoch)"],"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["***** Running training epoch 1 *****\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  from ipykernel import kernelapp as app\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 001 | Step 0625/3125 | Loss 0.0690 | Time 611.3532 | Learning rate = 8.000000000000001e-06 \n","\n","Epoch 001 | Step 1250/3125 | Loss 0.0687 | Time 1222.2252 | Learning rate = 6e-06 \n","\n","Epoch 001 | Step 1875/3125 | Loss 0.0686 | Time 1833.6001 | Learning rate = 4.000000000000001e-06 \n","\n","Epoch 001 | Step 2500/3125 | Loss 0.0685 | Time 2444.5682 | Learning rate = 2.0000000000000003e-06 \n","\n","Epoch 001 | Step 3125/3125 | Loss 0.0677 | Time 3055.7369 | Learning rate = 0.0 \n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"uSSkvEi5Zr1H"},"source":["1. 第一次batch_size=32,bert_lr=5e-5,其它lr=2.5e-4。训练集loss一直下降到0.03，测试集loss在第二个epoch就开始一直震荡，val_loss=0.2左右，acc在0.9460左右震荡\n","\n","2. 第二次batch_size=32,bert_lr=5e-5，其它lr=2.5e-4。bert的输出dropout概率由0.1升至0.3，scheduler加了一个epoch的warmup。（加了warmup那不是第二个epoch的lr最高，会不会更容易跳过最低点）\n","- 观察发现第一个epoch，lr最低到最高时，epoch内5次loss都明显下降(2.54-->1.02)。\n","- 第二个epoch内loss下降非常慢（0.2623-->0.2381）\n","- 惊鹊代码bert_lr=5e-5,其它是2e-5。test_batch_size=32，train_batch_size=4。dropout=0.15（bert和lstm两次）,没有warmup，epoch=2\n","3. 原来一直用的15000的数据，难怪准确率这么低"]},{"cell_type":"code","metadata":{"id":"AN5_2HNDiBAB"},"source":["#编写predict函数\n","def predict(model,data_loader):\n","  model.eval()\n","  test_pred = []\n","  for batch in data_loader:\n","    with torch.no_grad():\n","      input_ids=batch['input_ids'].to(device)\n","      attention_mask=batch['attention_mask'].to(device)\n","      token_type_ids=batch['token_type_ids'].to(device)\n","      logits=model(input_ids,attention_mask,token_type_ids)\n","\n","      y_pred=torch.argmax(logits,dim=1).detach().cpu().numpy().tolist()\n","      test_pred.extend(y_pred)\n","  return test_pred"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KfDv-o3Kl1ru"},"source":["#读取测试集\n","import pandas as pd\n","from datasets import load_dataset\n","test_df=pd.read_csv('./test_a.csv',sep='\\t',names=['text','label'])#读取测试集，新增一个空列名为label\n","\n","\n","#将训练数据中三个token换成标点\n","test_df['texts']=test_df['text'].map(lambda x:replacepunc(x))\n","#准备将text文本首尾截断，各取255tokens\n","def slipt2(x):\n","  ls=x.split(' ')\n","  le=len(ls)\n","  if le<511:\n","    return x\n","  else:\n","    return ' '.join(ls[:255]+ls[-255:])\n","test_df['summary']=test_df['texts'].apply(lambda x:slipt2(x))\n","test_df['label']=test_df['label'].fillna(0)#将测试集label全部填充为0,为了对应Dataset的读取格式"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":585},"id":"96xl_vbetY7P","executionInfo":{"status":"ok","timestamp":1636283973301,"user_tz":-480,"elapsed":516,"user":{"displayName":"张hongxu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01344108933923387301"}},"outputId":"cc4e087f-ef82-438f-d599-3235a23e1d73"},"source":["test_df=test_df.drop([0],axis=0)#第一行为原列索引，删除\n","test_df"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>label</th>\n","      <th>texts</th>\n","      <th>summary</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1</th>\n","      <td>5399 3117 1070 4321 4568 2621 5466 3772 4516 2...</td>\n","      <td>0.0</td>\n","      <td>5399 3117 1070 4321 4568 2621 5466 3772 4516 2...</td>\n","      <td>5399 3117 1070 4321 4568 2621 5466 3772 4516 2...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2491 4109 1757 7539 648 3695 3038 4490 23 7019...</td>\n","      <td>0.0</td>\n","      <td>2491 4109 1757 7539 ! 3695 3038 4490 23 7019 3...</td>\n","      <td>2491 4109 1757 7539 ! 3695 3038 4490 23 7019 3...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2673 5076 6835 2835 5948 5677 3247 4124 2465 5...</td>\n","      <td>0.0</td>\n","      <td>2673 5076 6835 2835 5948 5677 3247 4124 2465 5...</td>\n","      <td>2673 5076 6835 2835 5948 5677 3247 4124 2465 5...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4562 4893 2210 4761 3659 1324 2595 5949 4583 2...</td>\n","      <td>0.0</td>\n","      <td>4562 4893 2210 4761 3659 1324 2595 5949 4583 2...</td>\n","      <td>4562 4893 2210 4761 3659 1324 2595 5949 4583 2...</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>4269 7134 2614 1724 4464 1324 3370 3370 2106 2...</td>\n","      <td>0.0</td>\n","      <td>4269 7134 2614 1724 4464 1324 3370 3370 2106 2...</td>\n","      <td>4269 7134 2614 1724 4464 1324 3370 3370 2106 2...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>49996</th>\n","      <td>3725 4498 2282 1647 6293 4245 4498 3615 1141 2...</td>\n","      <td>0.0</td>\n","      <td>3725 4498 2282 1647 6293 4245 4498 3615 1141 2...</td>\n","      <td>3725 4498 2282 1647 6293 4245 4498 3615 1141 2...</td>\n","    </tr>\n","    <tr>\n","      <th>49997</th>\n","      <td>4811 465 3800 1394 3038 2376 2327 5165 3070 57...</td>\n","      <td>0.0</td>\n","      <td>4811 465 3800 1394 3038 2376 2327 5165 3070 57...</td>\n","      <td>4811 465 3800 1394 3038 2376 2327 5165 3070 57...</td>\n","    </tr>\n","    <tr>\n","      <th>49998</th>\n","      <td>5338 1952 3117 4109 299 6656 6654 3792 6831 21...</td>\n","      <td>0.0</td>\n","      <td>5338 1952 3117 4109 299 6656 6654 3792 6831 21...</td>\n","      <td>5338 1952 3117 4109 299 6656 6654 3792 6831 21...</td>\n","    </tr>\n","    <tr>\n","      <th>49999</th>\n","      <td>893 3469 5775 584 2490 4223 6569 6663 2124 168...</td>\n","      <td>0.0</td>\n","      <td>893 3469 5775 584 2490 4223 6569 6663 2124 168...</td>\n","      <td>893 3469 5775 584 2490 4223 6569 6663 2124 168...</td>\n","    </tr>\n","    <tr>\n","      <th>50000</th>\n","      <td>2400 4409 4412 2210 5122 4464 7186 2465 1327 9...</td>\n","      <td>0.0</td>\n","      <td>2400 4409 4412 2210 5122 4464 7186 2465 1327 9...</td>\n","      <td>2400 4409 4412 2210 5122 4464 7186 2465 1327 9...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>50000 rows × 4 columns</p>\n","</div>"],"text/plain":["                                                    text  ...                                            summary\n","1      5399 3117 1070 4321 4568 2621 5466 3772 4516 2...  ...  5399 3117 1070 4321 4568 2621 5466 3772 4516 2...\n","2      2491 4109 1757 7539 648 3695 3038 4490 23 7019...  ...  2491 4109 1757 7539 ! 3695 3038 4490 23 7019 3...\n","3      2673 5076 6835 2835 5948 5677 3247 4124 2465 5...  ...  2673 5076 6835 2835 5948 5677 3247 4124 2465 5...\n","4      4562 4893 2210 4761 3659 1324 2595 5949 4583 2...  ...  4562 4893 2210 4761 3659 1324 2595 5949 4583 2...\n","5      4269 7134 2614 1724 4464 1324 3370 3370 2106 2...  ...  4269 7134 2614 1724 4464 1324 3370 3370 2106 2...\n","...                                                  ...  ...                                                ...\n","49996  3725 4498 2282 1647 6293 4245 4498 3615 1141 2...  ...  3725 4498 2282 1647 6293 4245 4498 3615 1141 2...\n","49997  4811 465 3800 1394 3038 2376 2327 5165 3070 57...  ...  4811 465 3800 1394 3038 2376 2327 5165 3070 57...\n","49998  5338 1952 3117 4109 299 6656 6654 3792 6831 21...  ...  5338 1952 3117 4109 299 6656 6654 3792 6831 21...\n","49999  893 3469 5775 584 2490 4223 6569 6663 2124 168...  ...  893 3469 5775 584 2490 4223 6569 6663 2124 168...\n","50000  2400 4409 4412 2210 5122 4464 7186 2465 1327 9...  ...  2400 4409 4412 2210 5122 4464 7186 2465 1327 9...\n","\n","[50000 rows x 4 columns]"]},"metadata":{},"execution_count":25}]},{"cell_type":"code","metadata":{"id":"CEm7hIZV63zC"},"source":["#加载到datalodar并预处理\n","# 数据集读取\n","\n","from torch.utils.data import Dataset, DataLoader,TensorDataset\n","import torch\n","class XFeiDataset(Dataset):\n","  def __init__(self,encodings,labels):\n","    self.encodings=encodings\n","    self.labels=labels\n","  \n","  # 读取单个样本\n","  def __getitem__(self,idx):\n","    item={key:torch.tensor(val[idx]) for key,val in self.encodings.items()}\n","    item['labels']=torch.tensor(int(self.labels[idx]))\n","    return item\n","  \n","  def __len__(self):\n","    return len(self.labels)\n","test_encoding=fast_tokenizer(list(test_df['summary']),truncation=True,padding=True,return_tensors='pt')\n","test_dataset=XFeiDataset(test_encoding,list(test_df['label']))\n","test_loader=DataLoader(test_dataset,batch_size=64)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hJ_mdP_48V_R"},"source":[""]},{"cell_type":"code","metadata":{"id":"OwR5tJkRSHz-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636284348182,"user_tz":-480,"elapsed":283005,"user":{"displayName":"张hongxu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01344108933923387301"}},"outputId":"0b7f0658-0fb1-4d5c-ac9f-6559af218b51"},"source":["#用trainer预测结果并保存\n","torch.save(model.state_dict(),\"best_lstm_whole_4epoch\")\n","model.load_state_dict(torch.load(\"best_lstm_whole_4epoch\"))\n","model.to(device)\n","\n","predictions=predict(model,test_loader)\n","\n","pd.DataFrame({'label':predictions}).to_csv('bert_lstm_whole_4epoch.csv',index=None)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  del sys.path[0]\n"]}]},{"cell_type":"code","metadata":{"id":"QZlEV-Nb5ndh"},"source":["\"\"\"\n","#测试lstm模型输出\n","model2=nn.LSTM(\n","        input_size=512, \n","        hidden_size=1024 // 2,  # 1024\n","        batch_first=True,\n","        num_layers=3,\n","        dropout=0.1,  # 0.5\n","        bidirectional=True)\n","tests=torch.rand(5,3,512)\n","\n","output,(hn,cn)=model2(tests)\n","dhn=torch.cat([hn[-2],hn[-1]],dim=-1)#最后一层双向末端结果拼接，大小batch，dim*2\n","\n","print(output.shape)\n","print(hn.shape)\n","print(cn.shape)\n","#output输出是batch，len，dim*2（双向的话）\n","#hn和cn都是每个样本一个最后时刻的向量，为1，batch，dim。如果是多层就是num_layers*2，batch，dim\n","print(dhn.shape)\n","\"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rqB-fsVI5eL6","executionInfo":{"status":"ok","timestamp":1636683313772,"user_tz":-480,"elapsed":502,"user":{"displayName":"张hongxu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01344108933923387301"}},"outputId":"a109d14e-46ea-4446-8c93-151c474ae93e"},"source":["\"\"\"\n","import torch\n","a=torch.rand(3,5,5)\n","b=torch.rand(3,1,5)\n","print(a)\n","print(b)\n","\"\"\""],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[[0.9562, 0.7617, 0.2613, 0.6412, 0.1098],\n","         [0.5196, 0.6930, 0.0513, 0.7487, 0.5447],\n","         [0.6831, 0.9386, 0.9693, 0.4979, 0.9142],\n","         [0.5321, 0.8190, 0.6658, 0.8020, 0.0691],\n","         [0.3383, 0.3779, 0.2897, 0.1134, 0.2509]],\n","\n","        [[0.7673, 0.7032, 0.2328, 0.7086, 0.8448],\n","         [0.3034, 0.7003, 0.3047, 0.2293, 0.8905],\n","         [0.4470, 0.3945, 0.7205, 0.6552, 0.7908],\n","         [0.1026, 0.3110, 0.7278, 0.9898, 0.9549],\n","         [0.4184, 0.2673, 0.2727, 0.7099, 0.8876]],\n","\n","        [[0.7328, 0.5335, 0.2701, 0.1489, 0.8574],\n","         [0.2605, 0.9681, 0.9864, 0.8795, 0.3304],\n","         [0.0284, 0.7669, 0.3259, 0.7404, 0.7547],\n","         [0.1082, 0.5845, 0.8316, 0.4850, 0.4264],\n","         [0.3979, 0.6959, 0.1624, 0.4869, 0.2568]]])\n","tensor([[[0.3852, 0.3308, 0.1632, 0.3874, 0.7840]],\n","\n","        [[0.3321, 0.7246, 0.7355, 0.5728, 0.8681]],\n","\n","        [[0.4072, 0.4212, 0.6390, 0.5725, 0.8822]]])\n"]}]},{"cell_type":"code","metadata":{"id":"q6wphk2n7YzZ"},"source":[""],"execution_count":null,"outputs":[]}]}