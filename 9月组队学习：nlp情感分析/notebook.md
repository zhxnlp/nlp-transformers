æœ¬æ–‡æ¶‰åŠçš„jupter notebookåœ¨[ç¯‡ç« 4ä»£ç åº“ä¸­](https://github.com/datawhalechina/learn-nlp-with-transformers/tree/main/docs/%E7%AF%87%E7%AB%A04-%E4%BD%BF%E7%94%A8Transformers%E8%A7%A3%E5%86%B3NLP%E4%BB%BB%E5%8A%A1)ã€‚

ä¹Ÿç›´æ¥ä½¿ç”¨google colab notebookæ‰“å¼€æœ¬æ•™ç¨‹ï¼Œä¸‹è½½ç›¸å…³æ•°æ®é›†å’Œæ¨¡å‹ã€‚
å¦‚æœæ‚¨æ­£åœ¨googleçš„colabä¸­æ‰“å¼€è¿™ä¸ªnotebookï¼Œæ‚¨å¯èƒ½éœ€è¦å®‰è£…Transformerså’ŒğŸ¤—Datasetsåº“ã€‚å°†ä»¥ä¸‹å‘½ä»¤å–æ¶ˆæ³¨é‡Šå³å¯å®‰è£…ã€‚


```python
!pip install transformers datasets
```

    Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.10.2)
    Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (1.12.1)
    Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)
    Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)
    Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)
    Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)
    Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (5.4.1)
    Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)
    Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)
    Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)
    Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.2)
    Requirement already satisfied: huggingface-hub>=0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.17)
    Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)
    Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.12->transformers) (3.7.4.3)
    Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)
    Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)
    Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.7.4.post0)
    Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2021.8.1)
    Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (2.0.2)
    Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)
    Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)
    Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)
    Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)
    Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)
    Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)
    Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)
    Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.2.0)
    Requirement already satisfied: async-timeout<4.0,>=3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (3.0.1)
    Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (5.1.0)
    Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.6.3)
    Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)
    Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)
    Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)
    Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)
    Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)
    Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)
    

å¦‚æœæ‚¨æ­£åœ¨æœ¬åœ°æ‰“å¼€è¿™ä¸ªnotebookï¼Œè¯·ç¡®ä¿æ‚¨å·²ç»è¿›è¡Œä¸Šè¿°ä¾èµ–åŒ…çš„å®‰è£…ã€‚
æ‚¨ä¹Ÿå¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/huggingface/transformers/tree/master/examples/text-classification)æ‰¾åˆ°æœ¬notebookçš„å¤šGPUåˆ†å¸ƒå¼è®­ç»ƒç‰ˆæœ¬ã€‚


# å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œæ–‡æœ¬åˆ†ç±»

æˆ‘ä»¬å°†å±•ç¤ºå¦‚ä½•ä½¿ç”¨ [ğŸ¤— Transformers](https://github.com/huggingface/transformers)ä»£ç åº“ä¸­çš„æ¨¡å‹æ¥è§£å†³æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ï¼Œä»»åŠ¡æ¥æºäº[GLUE Benchmark](https://gluebenchmark.com/).

![Widget inference on a text classification task](https://github.com/huggingface/notebooks/blob/master/examples/images/text_classification.png?raw=1)

GLUEæ¦œå•åŒ…å«äº†9ä¸ªå¥å­çº§åˆ«çš„åˆ†ç±»ä»»åŠ¡ï¼Œåˆ†åˆ«æ˜¯ï¼š
- [CoLA](https://nyu-mll.github.io/CoLA/) (Corpus of Linguistic Acceptability) é‰´åˆ«ä¸€ä¸ªå¥å­æ˜¯å¦è¯­æ³•æ­£ç¡®.
- [MNLI](https://arxiv.org/abs/1704.05426) (Multi-Genre Natural Language Inference) ç»™å®šä¸€ä¸ªå‡è®¾ï¼Œåˆ¤æ–­å¦ä¸€ä¸ªå¥å­ä¸è¯¥å‡è®¾çš„å…³ç³»ï¼šentails, contradicts æˆ–è€… unrelatedã€‚
- [MRPC](https://www.microsoft.com/en-us/download/details.aspx?id=52398) (Microsoft Research Paraphrase Corpus) åˆ¤æ–­ä¸¤ä¸ªå¥å­æ˜¯å¦äº’ä¸ºparaphrases.
- [QNLI](https://rajpurkar.github.io/SQuAD-explorer/) (Question-answering Natural Language Inference) åˆ¤æ–­ç¬¬2å¥æ˜¯å¦åŒ…å«ç¬¬1å¥é—®é¢˜çš„ç­”æ¡ˆã€‚
- [QQP](https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs) (Quora Question Pairs2) åˆ¤æ–­ä¸¤ä¸ªé—®å¥æ˜¯å¦è¯­ä¹‰ç›¸åŒã€‚
- [RTE](https://aclweb.org/aclwiki/Recognizing_Textual_Entailment) (Recognizing Textual Entailment)åˆ¤æ–­ä¸€ä¸ªå¥å­æ˜¯å¦ä¸å‡è®¾æˆentailå…³ç³»ã€‚
- [SST-2](https://nlp.stanford.edu/sentiment/index.html) (Stanford Sentiment Treebank) åˆ¤æ–­ä¸€ä¸ªå¥å­çš„æƒ…æ„Ÿæ­£è´Ÿå‘.
- [STS-B](http://ixa2.si.ehu.es/stswiki/index.php/STSbenchmark) (Semantic Textual Similarity Benchmark) åˆ¤æ–­ä¸¤ä¸ªå¥å­çš„ç›¸ä¼¼æ€§ï¼ˆåˆ†æ•°ä¸º1-5åˆ†ï¼‰ã€‚
- [WNLI](https://cs.nyu.edu/faculty/davise/papers/WinogradSchemas/WS.html) (Winograd Natural Language Inference) Determine if a sentence with an anonymous pronoun and a sentence with this pronoun replaced are entailed or not. 

å¯¹äºä»¥ä¸Šä»»åŠ¡ï¼Œæˆ‘ä»¬å°†å±•ç¤ºå¦‚ä½•ä½¿ç”¨ç®€å•çš„Datasetåº“åŠ è½½æ•°æ®é›†ï¼ŒåŒæ—¶ä½¿ç”¨transformerä¸­çš„`Trainer`æ¥å£å¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚


```python
GLUE_TASKS = ["cola", "mnli", "mnli-mm", "mrpc", "qnli", "qqp", "rte", "sst2", "stsb", "wnli"]
```

æœ¬notebookç†è®ºä¸Šå¯ä»¥ä½¿ç”¨å„ç§å„æ ·çš„transformeræ¨¡å‹ï¼ˆ[æ¨¡å‹é¢æ¿](https://huggingface.co/models)ï¼‰ï¼Œè§£å†³ä»»ä½•æ–‡æœ¬åˆ†ç±»åˆ†ç±»ä»»åŠ¡ã€‚

å¦‚æœæ‚¨æ‰€å¤„ç†çš„ä»»åŠ¡æœ‰æ‰€ä¸åŒï¼Œå¤§æ¦‚ç‡åªéœ€è¦å¾ˆå°çš„æ”¹åŠ¨ä¾¿å¯ä»¥ä½¿ç”¨æœ¬notebookè¿›è¡Œå¤„ç†ã€‚åŒæ—¶ï¼Œæ‚¨åº”è¯¥æ ¹æ®æ‚¨çš„GPUæ˜¾å­˜æ¥è°ƒæ•´å¾®è°ƒè®­ç»ƒæ‰€éœ€è¦çš„btach sizeå¤§å°ï¼Œé¿å…æ˜¾å­˜æº¢å‡ºã€‚


```python
task = "sst2"
model_checkpoint = "distilbert-base-uncased"
batch_size = 16
```

## åŠ è½½æ•°æ®

æˆ‘ä»¬å°†ä¼šä½¿ç”¨[ğŸ¤— Datasets](https://github.com/huggingface/datasets)åº“æ¥åŠ è½½æ•°æ®å’Œå¯¹åº”çš„è¯„æµ‹æ–¹å¼ã€‚æ•°æ®åŠ è½½å’Œè¯„æµ‹æ–¹å¼åŠ è½½åªéœ€è¦ç®€å•ä½¿ç”¨`load_dataset`å’Œ`load_metric`å³å¯ã€‚


```python
from datasets import load_dataset, load_metric
```

é™¤äº†`mnli-mm`ä»¥å¤–ï¼Œå…¶ä»–ä»»åŠ¡éƒ½å¯ä»¥ç›´æ¥é€šè¿‡ä»»åŠ¡åå­—è¿›è¡ŒåŠ è½½ã€‚æ•°æ®åŠ è½½ä¹‹åä¼šè‡ªåŠ¨ç¼“å­˜ã€‚


```python


metric = load_metric('glue',task)
```

è¿™ä¸ª`datasets`å¯¹è±¡æœ¬èº«æ˜¯ä¸€ç§[`DatasetDict`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasetdict)æ•°æ®ç»“æ„. å¯¹äºè®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†ï¼Œåªéœ€è¦ä½¿ç”¨å¯¹åº”çš„keyï¼ˆtrainï¼Œvalidationï¼Œtestï¼‰å³å¯å¾—åˆ°ç›¸åº”çš„æ•°æ®ã€‚

ä¸ºäº†èƒ½å¤Ÿè¿›ä¸€æ­¥ç†è§£æ•°æ®é•¿ä»€ä¹ˆæ ·å­ï¼Œä¸‹é¢çš„å‡½æ•°å°†ä»æ•°æ®é›†é‡Œéšæœºé€‰æ‹©å‡ ä¸ªä¾‹å­è¿›è¡Œå±•ç¤ºã€‚

æ¯ä¸€ä¸ªæ–‡æœ¬åˆ†ç±»ä»»åŠ¡æ‰€å¯¹åº”çš„meticæœ‰æ‰€ä¸åŒï¼Œå…·ä½“å¦‚ä¸‹:

- for CoLA: [Matthews Correlation Coefficient](https://en.wikipedia.org/wiki/Matthews_correlation_coefficient)
- for MNLI (matched or mismatched): Accuracy
- for MRPC: Accuracy and [F1 score](https://en.wikipedia.org/wiki/F1_score)
- for QNLI: Accuracy
- for QQP: Accuracy and [F1 score](https://en.wikipedia.org/wiki/F1_score)
- for RTE: Accuracy
- for SST-2: Accuracy
- for STS-B: [Pearson Correlation Coefficient](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient) and [Spearman's_Rank_Correlation_Coefficient](https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient)
- for WNLI: Accuracy

æ‰€ä»¥ä¸€å®šè¦å°†metricå’Œä»»åŠ¡å¯¹é½

## æ•°æ®é¢„å¤„ç†


```python
import re
import nltk
import numpy as np 
import pandas as pd 
import matplotlib.pyplot as plt
from sklearn.metrics import f1_score
from collections import Counter
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords, wordnet

import os
for dirname, _, filenames in os.walk('/transformers'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

os.listdir("./transformers/nlp-getting-started/")
data_dir = "./transformers/"

data = pd.read_csv(data_dir + "nlp-getting-started/train.csv")
test_data = pd.read_csv(data_dir + "nlp-getting-started/test.csv")

data.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>id</th>
      <th>keyword</th>
      <th>location</th>
      <th>text</th>
      <th>target</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>Our Deeds are the Reason of this #earthquake M...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>4</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>Forest fire near La Ronge Sask. Canada</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>5</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>All residents asked to 'shelter in place' are ...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>6</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>13,000 people receive #wildfires evacuation or...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>7</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>Just got sent this photo from Ruby #Alaska as ...</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>




```python
train_data = data.drop("location", 1)
train_data.target.value_counts() 
```




    0    4342
    1    3271
    Name: target, dtype: int64




```python
def clean_text(text):
    
    new_text = text.lower() # lowercase the text
    new_text = re.sub(r"\w+\:\/\/([a-z]+)\.co\/\w+(\n)?", "", new_text) #remove urls
    new_text = re.sub(r"@[a-zA-Z0-9]+(?:;)*", "", new_text) # remove @s
    new_text = re.sub(r"#", "", new_text) # remove #s
    new_text = re.sub(r"[^a-z0-9A-Z]", " ", new_text) # remove non alphanumerics
    new_text = re.sub(r"[0-9]+[^\w+]", "", new_text) # remove words made wholy of digits
    new_text = re.sub(r"\b\w{1,2}\b", "", new_text) # remove words w/ 1 char
    new_text = re.sub(" +", " ", new_text) # remove multiple consecutive spaces
    
    new_text = new_text.strip() # remove leading/trailing whitespaces
    
    return new_text

train_data['keyword'].fillna('', inplace=True)
test_data['keyword'].fillna('', inplace=True)

train_data['text'] = train_data['text'].map(clean_text)
train_data['keyword'] = train_data['keyword'].map(clean_text)
test_data['text'] = test_data['text'].map(clean_text)
test_data['keyword'] = test_data['keyword'].map(clean_text)
```


```python
import nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
lm=nltk.stem.WordNetLemmatizer()
all_tokens=[item for _, value in tweets.items() for item in word_tokenize(value)]
#all_tokens_lm=[lm.lemmatize(t) for t in all_tokens]
all_tokens_lm=[lm.lemmatize(t) for t in all_tokens if t not in stopwords.words('english')]

N = len(all_tokens_lm)
V = len(set(all_tokens_lm))
         
print(f"There are {N} tokens after processing")
print(f"There are {V} unique tokens after processing")
```

    [nltk_data] Downloading package punkt to /root/nltk_data...
    [nltk_data]   Package punkt is already up-to-date!
    [nltk_data] Downloading package stopwords to /root/nltk_data...
    [nltk_data]   Package stopwords is already up-to-date!
    [nltk_data] Downloading package wordnet to /root/nltk_data...
    [nltk_data]   Package wordnet is already up-to-date!
    


    ---------------------------------------------------------------------------

    NameError                                 Traceback (most recent call last)

    <ipython-input-8-41c9ff8a385f> in <module>()
          4 nltk.download('wordnet')
          5 lm=nltk.stem.WordNetLemmatizer()
    ----> 6 all_tokens=[item for _, value in tweets.items() for item in word_tokenize(value)]
          7 #all_tokens_lm=[lm.lemmatize(t) for t in all_tokens]
          8 all_tokens_lm=[lm.lemmatize(t) for t in all_tokens if t not in stopwords.words('english')]
    

    NameError: name 'tweets' is not defined



```python
train_data.iloc[:6090]
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>id</th>
      <th>keyword</th>
      <th>text</th>
      <th>target</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td></td>
      <td>our deeds are the reason this earthquake may a...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>4</td>
      <td></td>
      <td>forest fire near ronge sask canada</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>5</td>
      <td></td>
      <td>all residents asked shelter place are being no...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>6</td>
      <td></td>
      <td>people receive wildfires evacuation orders cal...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>7</td>
      <td></td>
      <td>just got sent this photo from ruby alaska smok...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>6085</th>
      <td>8692</td>
      <td>sinking</td>
      <td>you feel like you are sinking low self image t...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>6086</th>
      <td>8693</td>
      <td>sinking</td>
      <td>after few years afloat pension plans start sin...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>6087</th>
      <td>8694</td>
      <td>sinking</td>
      <td>you feel like you are sinking unhappiness take...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>6088</th>
      <td>8695</td>
      <td>sinking</td>
      <td>with sinking music video career brooke hogan s...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>6089</th>
      <td>8696</td>
      <td>sinking</td>
      <td>feel bad for them can literally feel that feel...</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
<p>6090 rows Ã— 4 columns</p>
</div>




```python
from sklearn.model_selection import train_test_split
train_texts,val_texts,train_keyword,val_keyword,train_labels,val_labels = train_test_split(train_data['text'].tolist(),train_data['keyword'].tolist(),train_data['target'].tolist(),test_size=0.25)



```

åœ¨å°†æ•°æ®å–‚å…¥æ¨¡å‹ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦å¯¹æ•°æ®è¿›è¡Œé¢„å¤„ç†ã€‚é¢„å¤„ç†çš„å·¥å…·å«`Tokenizer`ã€‚`Tokenizer`é¦–å…ˆå¯¹è¾“å…¥è¿›è¡Œtokenizeï¼Œç„¶åå°†tokensè½¬åŒ–ä¸ºé¢„æ¨¡å‹ä¸­éœ€è¦å¯¹åº”çš„token IDï¼Œå†è½¬åŒ–ä¸ºæ¨¡å‹éœ€è¦çš„è¾“å…¥æ ¼å¼ã€‚

ä¸ºäº†è¾¾åˆ°æ•°æ®é¢„å¤„ç†çš„ç›®çš„ï¼Œæˆ‘ä»¬ä½¿ç”¨`AutoTokenizer.from_pretrained`æ–¹æ³•å®ä¾‹åŒ–æˆ‘ä»¬çš„tokenizerï¼Œè¿™æ ·å¯ä»¥ç¡®ä¿ï¼š

- æˆ‘ä»¬å¾—åˆ°ä¸€ä¸ªä¸é¢„è®­ç»ƒæ¨¡å‹ä¸€ä¸€å¯¹åº”çš„tokenizerã€‚
- ä½¿ç”¨æŒ‡å®šçš„æ¨¡å‹checkpointå¯¹åº”çš„tokenizerçš„æ—¶å€™ï¼Œæˆ‘ä»¬ä¹Ÿä¸‹è½½äº†æ¨¡å‹éœ€è¦çš„è¯è¡¨åº“vocabularyï¼Œå‡†ç¡®æ¥è¯´æ˜¯tokens vocabularyã€‚

è¿™ä¸ªè¢«ä¸‹è½½çš„tokens vocabularyä¼šè¢«ç¼“å­˜èµ·æ¥ï¼Œä»è€Œå†æ¬¡ä½¿ç”¨çš„æ—¶å€™ä¸ä¼šé‡æ–°ä¸‹è½½ã€‚


```python
print(train_data['text'].str.split().str.len().describe())
print(train_data['keyword'].str.split().str.len().describe())
print(test_data['text'].str.split().str.len().describe())
print(test_data['keyword'].str.split().str.len().describe())
```


```python
from transformers import AutoTokenizer
    
tokenizer=AutoTokenizer.from_pretrained(model_checkpoint,use_fast=True)
```

æ³¨æ„ï¼š`use_fast=True`è¦æ±‚tokenizerå¿…é¡»æ˜¯transformers.PreTrainedTokenizerFastç±»å‹ï¼Œå› ä¸ºæˆ‘ä»¬åœ¨é¢„å¤„ç†çš„æ—¶å€™éœ€è¦ç”¨åˆ°fast tokenizerçš„ä¸€äº›ç‰¹æ®Šç‰¹æ€§ï¼ˆæ¯”å¦‚å¤šçº¿ç¨‹å¿«é€Ÿtokenizerï¼‰ã€‚å¦‚æœå¯¹åº”çš„æ¨¡å‹æ²¡æœ‰fast tokenizerï¼Œå»æ‰è¿™ä¸ªé€‰é¡¹å³å¯ã€‚

å‡ ä¹æ‰€æœ‰æ¨¡å‹å¯¹åº”çš„tokenizeréƒ½æœ‰å¯¹åº”çš„fast tokenizerã€‚æˆ‘ä»¬å¯ä»¥åœ¨[æ¨¡å‹tokenizerå¯¹åº”è¡¨](https://huggingface.co/transformers/index.html#bigtable)é‡ŒæŸ¥çœ‹æ‰€æœ‰é¢„è®­ç»ƒæ¨¡å‹å¯¹åº”çš„tokenizeræ‰€æ‹¥æœ‰çš„ç‰¹ç‚¹ã€‚


```python
training_text_encodings=tokenizer(train_texts,padding=True,truncation=True)
val_text_encodings=tokenizer(val_texts,padding=True,truncation=True)

training_keyword_encodings=tokenizer(train_keyword,padding=True,truncation=True)
val_keyword_encodings=tokenizer(val_keyword,padding=True,truncation=True)
```


```python
class DatasetCls():
    def __init__(self, text_encodings, keyword_encodings, labels):
        self.text_encodings = text_encodings
        self.keyword_encodings = keyword_encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {f"text_{key}" : torch.tensor(val[idx]) for key, val in self.text_encodings.items()}
        item.update({f"keyword_{key}" : torch.tensor(val[idx]) for key, val in self.text_encodings.items()})
        item['labels'] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)
```


```python
from torch.utils.data import DataLoader
train_dataset = DatasetCls(training_text_encodings,training_keyword_encodings,train_labels)
val_dataset = DatasetCls(val_text_encodings,val_keyword_encodings,val_labels)

train_loader = DataLoader(train_dataset,batch_size=16,shuffle=True)
eval_loader = DataLoader(val_dataset,batch_size=16)
```

éšåå°†é¢„å¤„ç†çš„ä»£ç æ”¾åˆ°ä¸€ä¸ªå‡½æ•°ä¸­ï¼š


```python
def preprocess_function(examples):    
  return tokenizer(examples['text'],examples['keyword'],truncation=True)
    
```

æ¥ä¸‹æ¥å¯¹æ•°æ®é›†datasetsé‡Œé¢çš„æ‰€æœ‰æ ·æœ¬è¿›è¡Œé¢„å¤„ç†ï¼Œå¤„ç†çš„æ–¹å¼æ˜¯ä½¿ç”¨mapå‡½æ•°ï¼Œå°†é¢„å¤„ç†å‡½æ•°prepare_train_featuresåº”ç”¨åˆ°ï¼ˆmap)æ‰€æœ‰æ ·æœ¬ä¸Šã€‚


```python
train_datasets=preprocess_function(train_set)
#eval_datasets=validation_set.map(preprocess_function,batched=True)
```


    ---------------------------------------------------------------------------

    ValueError                                Traceback (most recent call last)

    <ipython-input-75-8e57f1a3e6d3> in <module>()
    ----> 1 train_datasets=preprocess_function(train_set)
          2 #eval_datasets=validation_set.map(preprocess_function,batched=True)
    

    <ipython-input-51-0f8c637e0fb9> in preprocess_function(examples)
          1 def preprocess_function(examples):
    ----> 2   return tokenizer(examples['text'],examples['keyword'],truncation=True)
          3 
    

    /usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py in __call__(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)
       2355         if not _is_valid_text_input(text):
       2356             raise ValueError(
    -> 2357                 "text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) "
       2358                 "or `List[List[str]]` (batch of pretokenized examples)."
       2359             )
    

    ValueError: text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).



æ›´å¥½çš„æ˜¯ï¼Œè¿”å›çš„ç»“æœä¼šè‡ªåŠ¨è¢«ç¼“å­˜ï¼Œé¿å…ä¸‹æ¬¡å¤„ç†çš„æ—¶å€™é‡æ–°è®¡ç®—ï¼ˆä½†æ˜¯ä¹Ÿè¦æ³¨æ„ï¼Œå¦‚æœè¾“å…¥æœ‰æ”¹åŠ¨ï¼Œå¯èƒ½ä¼šè¢«ç¼“å­˜å½±å“ï¼ï¼‰ã€‚datasetsåº“å‡½æ•°ä¼šå¯¹è¾“å…¥çš„å‚æ•°è¿›è¡Œæ£€æµ‹ï¼Œåˆ¤æ–­æ˜¯å¦æœ‰å˜åŒ–ï¼Œå¦‚æœæ²¡æœ‰å˜åŒ–å°±ä½¿ç”¨ç¼“å­˜æ•°æ®ï¼Œå¦‚æœæœ‰å˜åŒ–å°±é‡æ–°å¤„ç†ã€‚ä½†å¦‚æœè¾“å…¥å‚æ•°ä¸å˜ï¼Œæƒ³æ”¹å˜è¾“å…¥çš„æ—¶å€™ï¼Œæœ€å¥½æ¸…ç†è°ƒè¿™ä¸ªç¼“å­˜ã€‚æ¸…ç†çš„æ–¹å¼æ˜¯ä½¿ç”¨`load_from_cache_file=False`å‚æ•°ã€‚å¦å¤–ï¼Œä¸Šé¢ä½¿ç”¨åˆ°çš„`batched=True`è¿™ä¸ªå‚æ•°æ˜¯tokenizerçš„ç‰¹ç‚¹ï¼Œä»¥ä¸ºè¿™ä¼šä½¿ç”¨å¤šçº¿ç¨‹åŒæ—¶å¹¶è¡Œå¯¹è¾“å…¥è¿›è¡Œå¤„ç†ã€‚

## å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹

æ—¢ç„¶æ•°æ®å·²ç»å‡†å¤‡å¥½äº†ï¼Œç°åœ¨æˆ‘ä»¬éœ€è¦ä¸‹è½½å¹¶åŠ è½½æˆ‘ä»¬çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œç„¶åå¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹ã€‚æ—¢ç„¶æˆ‘ä»¬æ˜¯åšseq2seqä»»åŠ¡ï¼Œé‚£ä¹ˆæˆ‘ä»¬éœ€è¦ä¸€ä¸ªèƒ½è§£å†³è¿™ä¸ªä»»åŠ¡çš„æ¨¡å‹ç±»ã€‚æˆ‘ä»¬ä½¿ç”¨`AutoModelForSequenceClassification` è¿™ä¸ªç±»ã€‚å’Œtokenizerç›¸ä¼¼ï¼Œ`from_pretrained`æ–¹æ³•åŒæ ·å¯ä»¥å¸®åŠ©æˆ‘ä»¬ä¸‹è½½å¹¶åŠ è½½æ¨¡å‹ï¼ŒåŒæ—¶ä¹Ÿä¼šå¯¹æ¨¡å‹è¿›è¡Œç¼“å­˜ï¼Œå°±ä¸ä¼šé‡å¤ä¸‹è½½æ¨¡å‹å•¦ã€‚

éœ€è¦æ³¨æ„çš„æ˜¯ï¼šSTS-Bæ˜¯ä¸€ä¸ªå›å½’é—®é¢˜ï¼ŒMNLIæ˜¯ä¸€ä¸ª3åˆ†ç±»é—®é¢˜ï¼š



```python
from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer

#num_labels = 3 if task.startswith("mnli") else 1 if task=="stsb" else 2
model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint,num_labels=2)
```

    Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight']
    - This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
    - This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
    Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']
    You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
    


```python
from transformers import AdamW

optimizer = AdamW(model.parameters(), lr=5e-5)

from transformers import get_scheduler

num_epochs = 5
num_training_steps = num_epochs * len(train_loader)
lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps
)
```


```python
import torch

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
model.to(device)

from tqdm.auto import tqdm

progress_bar = tqdm(range(num_training_steps))

model.train()
for epoch in range(num_epochs):
    for batch in train_loader:
        batch = {k: v.to(device) for k, v in batch.items()}
        outputs = model(**batch)
        loss = outputs.loss
        loss.backward()

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)


```


      0%|          | 0/1785 [00:00<?, ?it/s]



    ---------------------------------------------------------------------------

    TypeError                                 Traceback (most recent call last)

    <ipython-input-16-44f4409bc57e> in <module>()
         12     for batch in train_loader:
         13         batch = {k: v.to(device) for k, v in batch.items()}
    ---> 14         outputs = model(**batch)
         15         loss = outputs.loss
         16         loss.backward()
    

    /usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
       1049         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
       1050                 or _global_forward_hooks or _global_forward_pre_hooks):
    -> 1051             return forward_call(*input, **kwargs)
       1052         # Do not call functions when jit is used
       1053         full_backward_hooks, non_full_backward_hooks = [], []
    

    TypeError: forward() got an unexpected keyword argument 'text_input_ids'



```python
metric_name="accuracy"

args = TrainingArguments(
    "nlp-getting-started",
    evaluation_strategy = "epoch",
    save_strategy = "epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    num_train_epochs=5,
    weight_decay=0.01,
    load_best_model_at_end=True,
    metric_for_best_model=metric_name,
)
```

ä¸Šé¢evaluation_strategy = "epoch"å‚æ•°å‘Šè¯‰è®­ç»ƒä»£ç ï¼šæˆ‘ä»¬æ¯ä¸ªepcohä¼šåšä¸€æ¬¡éªŒè¯è¯„ä¼°ã€‚

ä¸Šé¢batch_sizeåœ¨è¿™ä¸ªnotebookä¹‹å‰å®šä¹‰å¥½äº†ã€‚

æœ€åï¼Œç”±äºä¸åŒçš„ä»»åŠ¡éœ€è¦ä¸åŒçš„è¯„æµ‹æŒ‡æ ‡ï¼Œæˆ‘ä»¬å®šä¸€ä¸ªå‡½æ•°æ¥æ ¹æ®ä»»åŠ¡åå­—å¾—åˆ°è¯„ä»·æ–¹æ³•:


```python
def compute_metrics(eval_pred):
    predictions, labels=eval_pred
    if task != "stsb":
        predictions=np.argmax(predictions, axis=1)
    else:
        predictions=predictions[:,0]
    return metric.compute(predictions=predictions, references=labels)
```

å…¨éƒ¨ä¼ ç»™ `Trainer`:


```python
#validation_key = "validation_mismatched" if task == "mnli-mm" else "validation_matched" if task == "mnli" else "validation"
trainer = Trainer(
    model,
    args,
    train_dataset=train_loader,
    eval_dataset=eval_loader,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)
```


```python
import numpy as np
import pandas as pd
```

å¼€å§‹è®­ç»ƒ:


```python
trainer.train()
```

    ***** Running training *****
      Num examples = 357
      Num Epochs = 5
      Instantaneous batch size per device = 16
      Total train batch size (w. parallel, distributed & accumulation) = 16
      Gradient Accumulation steps = 1
      Total optimization steps = 115
    


    ---------------------------------------------------------------------------

    TypeError                                 Traceback (most recent call last)

    <ipython-input-21-3435b262f1ae> in <module>()
    ----> 1 trainer.train()
    

    /usr/local/lib/python3.7/dist-packages/transformers/trainer.py in train(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)
       1256             self.control = self.callback_handler.on_epoch_begin(args, self.state, self.control)
       1257 
    -> 1258             for step, inputs in enumerate(epoch_iterator):
       1259 
       1260                 # Skip past any already trained steps if resuming training
    

    /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py in __next__(self)
        519             if self._sampler_iter is None:
        520                 self._reset()
    --> 521             data = self._next_data()
        522             self._num_yielded += 1
        523             if self._dataset_kind == _DatasetKind.Iterable and \
    

    /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py in _next_data(self)
        559     def _next_data(self):
        560         index = self._next_index()  # may raise StopIteration
    --> 561         data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
        562         if self._pin_memory:
        563             data = _utils.pin_memory.pin_memory(data)
    

    /usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py in fetch(self, possibly_batched_index)
         42     def fetch(self, possibly_batched_index):
         43         if self.auto_collation:
    ---> 44             data = [self.dataset[idx] for idx in possibly_batched_index]
         45         else:
         46             data = self.dataset[possibly_batched_index]
    

    /usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py in <listcomp>(.0)
         42     def fetch(self, possibly_batched_index):
         43         if self.auto_collation:
    ---> 44             data = [self.dataset[idx] for idx in possibly_batched_index]
         45         else:
         46             data = self.dataset[possibly_batched_index]
    

    TypeError: 'DataLoader' object is not subscriptable


è®­ç»ƒå®Œæˆåè¿›è¡Œè¯„ä¼°:


```python
trainer.evaluate()
```

    The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence, idx.
    ***** Running Evaluation *****
      Num examples = 1043
      Batch size = 16
    



<div>

  <progress value='66' max='66' style='width:300px; height:20px; vertical-align: middle;'></progress>
  [66/66 00:02]
</div>






    {'epoch': 5.0,
     'eval_loss': 0.7508974671363831,
     'eval_matthews_correlation': 0.5347185537666785,
     'eval_runtime': 2.1314,
     'eval_samples_per_second': 489.347,
     'eval_steps_per_second': 30.965}



To see how your model fared you can compare it to the [GLUE Benchmark leaderboard](https://gluebenchmark.com/leaderboard).


```python
tokenizer.save_pretrained("C/sentences_classfication")
model.save_pretrained("C/sentences_classfication")
```

    tokenizer config file saved in C/sentences_classfication/tokenizer_config.json
    Special tokens file saved in C/sentences_classfication/special_tokens_map.json
    Configuration saved in C/sentences_classfication/config.json
    Model weights saved in C/sentences_classfication/pytorch_model.bin
    


```python
from google.colab import drive
drive.mount('/content/drive')
```

    Mounted at /content/drive
    

## è¶…å‚æ•°æœç´¢

`Trainer`åŒæ ·æ”¯æŒè¶…å‚æœç´¢ï¼Œä½¿ç”¨[optuna](https://optuna.org/) or [Ray Tune](https://docs.ray.io/en/latest/tune/)ä»£ç åº“ã€‚

åæ³¨é‡Šä¸‹é¢ä¸¤è¡Œå®‰è£…ä¾èµ–ï¼š


```python
! pip install optuna
! pip install ray[tune]
```

è¶…å‚æœç´¢æ—¶ï¼Œ`Trainer`å°†ä¼šè¿”å›å¤šä¸ªè®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œæ‰€ä»¥éœ€è¦ä¼ å…¥ä¸€ä¸ªå®šä¹‰å¥½çš„æ¨¡å‹ä»è€Œè®©`Trainer`å¯ä»¥ä¸æ–­é‡æ–°åˆå§‹åŒ–è¯¥ä¼ å…¥çš„æ¨¡å‹ï¼š


```python
def model_init():
    return AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)
```

å’Œä¹‹å‰è°ƒç”¨ `Trainer`ç±»ä¼¼:


```python
trainer = Trainer(
    model_init=model_init,
    args=args,
    train_dataset=encoded_dataset["train"],
    eval_dataset=encoded_dataset[validation_key],
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)
```

    loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.d423bdf2f58dc8b77d5f5d18028d7ae4a72dcfd8f468e81fe979ada957a8c361
    Model config DistilBertConfig {
      "activation": "gelu",
      "architectures": [
        "DistilBertForMaskedLM"
      ],
      "attention_dropout": 0.1,
      "dim": 768,
      "dropout": 0.1,
      "hidden_dim": 3072,
      "initializer_range": 0.02,
      "max_position_embeddings": 512,
      "model_type": "distilbert",
      "n_heads": 12,
      "n_layers": 6,
      "pad_token_id": 0,
      "qa_dropout": 0.1,
      "seq_classif_dropout": 0.2,
      "sinusoidal_pos_embds": false,
      "tie_weights_": true,
      "transformers_version": "4.9.1",
      "vocab_size": 30522
    }
    
    loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a
    Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight']
    - This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
    - This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
    Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias', 'classifier.bias']
    You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
    

è°ƒç”¨æ–¹æ³•`hyperparameter_search`ã€‚æ³¨æ„ï¼Œè¿™ä¸ªè¿‡ç¨‹å¯èƒ½å¾ˆä¹…ï¼Œæˆ‘ä»¬å¯ä»¥å…ˆç”¨éƒ¨åˆ†æ•°æ®é›†è¿›è¡Œè¶…å‚æœç´¢ï¼Œå†è¿›è¡Œå…¨é‡è®­ç»ƒã€‚
æ¯”å¦‚ä½¿ç”¨1/10çš„æ•°æ®è¿›è¡Œæœç´¢ï¼š


```python
best_run = trainer.hyperparameter_search(n_trials=10, direction="maximize")
```

`hyperparameter_search`ä¼šè¿”å›æ•ˆæœæœ€å¥½çš„æ¨¡å‹ç›¸å…³çš„å‚æ•°ï¼š


```python
best_run
```

å°†`Trainner`è®¾ç½®ä¸ºæœç´¢åˆ°çš„æœ€å¥½å‚æ•°ï¼Œè¿›è¡Œè®­ç»ƒï¼š


```python
for n, v in best_run.hyperparameters.items():
    setattr(trainer.args, n, v)

trainer.train()
```

æœ€ååˆ«å¿˜äº†ï¼ŒæŸ¥çœ‹å¦‚ä½•ä¸Šä¼ æ¨¡å‹ ï¼Œä¸Šä¼ æ¨¡å‹åˆ°](https://huggingface.co/transformers/model_sharing.html) åˆ°[ğŸ¤— Model Hub](https://huggingface.co/models)ã€‚éšåæ‚¨å°±å¯ä»¥åƒè¿™ä¸ªnotebookä¸€å¼€å§‹ä¸€æ ·ï¼Œç›´æ¥ç”¨æ¨¡å‹åå­—å°±èƒ½ä½¿ç”¨æ‚¨è‡ªå·±ä¸Šä¼ çš„æ¨¡å‹å•¦ã€‚


```python
zip -q -r checkpoint-2675.zip /content/test-glue/checkpoint-2675
```


      File "<ipython-input-28-bcb0775fb004>", line 1
        zip -q -r checkpoint-2675.zip /content/test-glue/checkpoint-2675
                           ^
    SyntaxError: invalid syntax
    



```python
from google.colab import drive
drive.mount('/content/drive')

```

    Mounted at /content/drive
    


```python
import os
os.chdir("/content/drive/MyDrive")
```


```python

```
