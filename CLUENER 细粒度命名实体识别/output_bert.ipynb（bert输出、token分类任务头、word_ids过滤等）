{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"output_bert.ipynb（bert输出、token分类任务头、word_ids过滤等）","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyO7ld4iOb8f3uPg1RNElBkR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"4d8848f64a9640adb9b273b4ffca7efe":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_864078bec70f4dc5960cc135cf875771","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_3469de887d344cce9f2611251f6c69bf","IPY_MODEL_8a4cbece899c44fbbb07a5f1cb9f6d2c","IPY_MODEL_f6c62823af8b4cb98e6e903a9da4a8d6"]}},"864078bec70f4dc5960cc135cf875771":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3469de887d344cce9f2611251f6c69bf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_fd093fcd0a24484e8ec08436a496e970","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_25750d755efb49f982e1dcea29cf3b40"}},"8a4cbece899c44fbbb07a5f1cb9f6d2c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_e1c9ad732838455eb0a83ce5d4f95e30","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":1306484351,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1306484351,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_8b665e34c7f64ac08478c588152fb158"}},"f6c62823af8b4cb98e6e903a9da4a8d6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_1867b1b1a2fe4e1eadbb40998159f40c","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1.22G/1.22G [00:41&lt;00:00, 29.1MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_5e82e33f12724c1d8b902d7f9431beb0"}},"fd093fcd0a24484e8ec08436a496e970":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"25750d755efb49f982e1dcea29cf3b40":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e1c9ad732838455eb0a83ce5d4f95e30":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"8b665e34c7f64ac08478c588152fb158":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1867b1b1a2fe4e1eadbb40998159f40c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"5e82e33f12724c1d8b902d7f9431beb0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hw0bsFiqWXy6","executionInfo":{"status":"ok","timestamp":1636550333753,"user_tz":-480,"elapsed":18877,"user":{"displayName":"张hongxu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01344108933923387301"}},"outputId":"679af940-1713-4411-9d7e-63ffe41c86d7"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"id":"e01TmJXPXERL"},"source":["import os\n","os.chdir('/content/drive/MyDrive/chinese task/CLUENER2020')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zalA7BN8XOEo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636550349426,"user_tz":-480,"elapsed":11783,"user":{"displayName":"张hongxu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01344108933923387301"}},"outputId":"07a395e5-bd14-43d7-e03f-a1f819717231"},"source":["#安装\n","!pip install transformers datasets"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.12.3-py3-none-any.whl (3.1 MB)\n","\u001b[K     |████████████████████████████████| 3.1 MB 24.4 MB/s \n","\u001b[?25hCollecting datasets\n","  Downloading datasets-1.15.1-py3-none-any.whl (290 kB)\n","\u001b[K     |████████████████████████████████| 290 kB 46.7 MB/s \n","\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 45.6 MB/s \n","\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.1.2-py3-none-any.whl (59 kB)\n","\u001b[K     |████████████████████████████████| 59 kB 7.4 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 35.4 MB/s \n","\u001b[?25hCollecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 37.4 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.3.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n","Requirement already satisfied: pyparsing<3,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n","Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n","Collecting aiohttp\n","  Downloading aiohttp-3.8.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n","\u001b[K     |████████████████████████████████| 1.1 MB 37.0 MB/s \n","\u001b[?25hCollecting xxhash\n","  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n","\u001b[K     |████████████████████████████████| 243 kB 38.2 MB/s \n","\u001b[?25hRequirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n","Collecting fsspec[http]>=2021.05.0\n","  Downloading fsspec-2021.11.0-py3-none-any.whl (132 kB)\n","\u001b[K     |████████████████████████████████| 132 kB 77.0 MB/s \n","\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.2.0)\n","Collecting yarl<2.0,>=1.0\n","  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n","\u001b[K     |████████████████████████████████| 271 kB 60.4 MB/s \n","\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n","  Downloading async_timeout-4.0.0-py3-none-any.whl (6.1 kB)\n","Collecting frozenlist>=1.1.1\n","  Downloading frozenlist-1.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (192 kB)\n","\u001b[K     |████████████████████████████████| 192 kB 61.3 MB/s \n","\u001b[?25hRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.7)\n","Collecting aiosignal>=1.1.2\n","  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n","Collecting multidict<7.0,>=4.5\n","  Downloading multidict-5.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (160 kB)\n","\u001b[K     |████████████████████████████████| 160 kB 60.5 MB/s \n","\u001b[?25hCollecting asynctest==0.13.0\n","  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Installing collected packages: multidict, frozenlist, yarl, asynctest, async-timeout, aiosignal, pyyaml, fsspec, aiohttp, xxhash, tokenizers, sacremoses, huggingface-hub, transformers, datasets\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed aiohttp-3.8.0 aiosignal-1.2.0 async-timeout-4.0.0 asynctest-0.13.0 datasets-1.15.1 frozenlist-1.2.0 fsspec-2021.11.0 huggingface-hub-0.1.2 multidict-5.2.0 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.12.3 xxhash-2.0.2 yarl-1.7.2\n"]}]},{"cell_type":"code","metadata":{"id":"0a0Y29QSXciS"},"source":["import os\n","import json\n","import logging\n","import numpy as np\n","import pandas as pd\n","import config\n","\n","from sklearn.model_selection import train_test_split\n","from torch.utils.data import DataLoader"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4DGMH-td8_Sn","executionInfo":{"status":"ok","timestamp":1636550367566,"user_tz":-480,"elapsed":808,"user":{"displayName":"张hongxu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01344108933923387301"}},"outputId":"0ca6c2e5-5f9d-4deb-9587-c4d838f42129"},"source":["#加载处理完的npz数据集\n","#不加allow_pickle=True会报错Object arrays cannot be loaded when allow_pickle=False，numpy新版本中默认为False。\n","train_data=np.load('./data/train.npz',allow_pickle=True)\n","val_data=np.load('./data/dev.npz',allow_pickle=True)\n","val_data.files"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['words', 'labels']"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":417},"id":"W6F-6qmWgJRr","executionInfo":{"status":"ok","timestamp":1636550370937,"user_tz":-480,"elapsed":864,"user":{"displayName":"张hongxu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01344108933923387301"}},"outputId":"5e21b8d7-7896-4517-b8db-bf3f1f55db02"},"source":["#转换为dataframe格式\n","import pandas as pd\n","#补个随机frac\n","train_df=pd.concat([pd.DataFrame(train_data['words'],columns=['words']),\n","          pd.DataFrame(train_data['labels'],columns=['labels'])],axis=1)\n","\n","val_df=pd.concat([pd.DataFrame(val_data['words'],columns=['words']),\n","          pd.DataFrame(val_data['labels'],columns=['labels'])],axis=1)\n","\n","\n","#将BIOS标签转换为数字索引，此时word和labels已经对齐了\n","def trans(labels):\n","  labels=list(labels)\n","  nums=[]\n","  for label in labels:\n","    nums.append(config.label2id[label])\n","  return nums\n","    \n","train_df['labels']=train_df['labels'].map(lambda x: trans(x))\n","val_df['labels']=val_df['labels'].map(lambda x: trans(x))\n","val_df"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>words</th>\n","      <th>labels</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>[彭, 小, 军, 认, 为, ，, 国, 内, 银, 行, 现, 在, 走, 的, 是, ...</td>\n","      <td>[7, 17, 17, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>[温, 格, 的, 球, 队, 终, 于, 又, 踢, 了, 一, 场, 经, 典, 的, ...</td>\n","      <td>[7, 17, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>[突, 袭, 黑, 暗, 雅, 典, 娜, 》, 中, R, i, d, d, i, c, ...</td>\n","      <td>[4, 14, 14, 14, 14, 14, 14, 14, 0, 7, 17, 17, ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>[郑, 阿, 姨, 就, 赶, 到, 文, 汇, 路, 排, 队, 拿, 钱, ，, 希, ...</td>\n","      <td>[0, 0, 0, 0, 0, 0, 1, 11, 11, 0, 0, 0, 0, 0, 0...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>[我, 想, 站, 在, 雪, 山, 脚, 下, 你, 会, 被, 那, 巍, 峨, 的, ...</td>\n","      <td>[0, 0, 0, 0, 10, 20, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1338</th>\n","      <td>[在, 这, 个, 非, 常, 喜, 庆, 的, 日, 子, 里, ，, 我, 们, 首, ...</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>1339</th>\n","      <td>[姜, 哲, 中, ：, 公, 共, 之, 敌, 1, -, 1, 》, 、, 《, 神, ...</td>\n","      <td>[6, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16...</td>\n","    </tr>\n","    <tr>\n","      <th>1340</th>\n","      <td>[目, 前, ，, 日, 本, 松, 山, 海, 上, 保, 安, 部, 正, 在, 就, ...</td>\n","      <td>[0, 0, 0, 5, 15, 15, 15, 15, 15, 15, 15, 15, 0...</td>\n","    </tr>\n","    <tr>\n","      <th>1341</th>\n","      <td>[也, 就, 是, 说, 英, 国, 人, 在, 世, 博, 会, 上, 的, 英, 国, ...</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 10, 20, 20, 0, 0, 0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>1342</th>\n","      <td>[另, 外, 意, 大, 利, 的, P, l, a, y, G, e, n, e, r, ...</td>\n","      <td>[0, 0, 0, 0, 0, 0, 2, 12, 12, 12, 12, 12, 12, ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1343 rows × 2 columns</p>\n","</div>"],"text/plain":["                                                  words                                             labels\n","0     [彭, 小, 军, 认, 为, ，, 国, 内, 银, 行, 现, 在, 走, 的, 是, ...  [7, 17, 17, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...\n","1     [温, 格, 的, 球, 队, 终, 于, 又, 踢, 了, 一, 场, 经, 典, 的, ...  [7, 17, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n","2     [突, 袭, 黑, 暗, 雅, 典, 娜, 》, 中, R, i, d, d, i, c, ...  [4, 14, 14, 14, 14, 14, 14, 14, 0, 7, 17, 17, ...\n","3     [郑, 阿, 姨, 就, 赶, 到, 文, 汇, 路, 排, 队, 拿, 钱, ，, 希, ...  [0, 0, 0, 0, 0, 0, 1, 11, 11, 0, 0, 0, 0, 0, 0...\n","4     [我, 想, 站, 在, 雪, 山, 脚, 下, 你, 会, 被, 那, 巍, 峨, 的, ...  [0, 0, 0, 0, 10, 20, 0, 0, 0, 0, 0, 0, 0, 0, 0...\n","...                                                 ...                                                ...\n","1338  [在, 这, 个, 非, 常, 喜, 庆, 的, 日, 子, 里, ，, 我, 们, 首, ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n","1339  [姜, 哲, 中, ：, 公, 共, 之, 敌, 1, -, 1, 》, 、, 《, 神, ...  [6, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16...\n","1340  [目, 前, ，, 日, 本, 松, 山, 海, 上, 保, 安, 部, 正, 在, 就, ...  [0, 0, 0, 5, 15, 15, 15, 15, 15, 15, 15, 15, 0...\n","1341  [也, 就, 是, 说, 英, 国, 人, 在, 世, 博, 会, 上, 的, 英, 国, ...  [0, 0, 0, 0, 0, 0, 0, 0, 10, 20, 20, 0, 0, 0, ...\n","1342  [另, 外, 意, 大, 利, 的, P, l, a, y, G, e, n, e, r, ...  [0, 0, 0, 0, 0, 0, 2, 12, 12, 12, 12, 12, 12, ...\n","\n","[1343 rows x 2 columns]"]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"KzH2PsAaQY_s"},"source":["##直接将数据集进行分词预处理，并且将labels填充到同一长度。"]},{"cell_type":"code","metadata":{"id":"WdNZ8-PrL1yt"},"source":["for batch in train_dataset[:5]:\n","    print(batch)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gZvx4mv7Qqb2"},"source":["label读取错误，重写dataset，去掉label的读取"]},{"cell_type":"code","metadata":{"id":"jt6PsrZKJ_i7"},"source":["tokenized_val_ds['labels'][:10]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BmphIPdDCKrB"},"source":["# 数据集读取\n","batch_size=4\n","import torch\n","class XFeiDataset(Dataset):\n","  def __init__(self,encodings,labels):\n","    self.encodings=encodings\n","    self.labels=labels\n","  \n","  # 读取单个样本\n","  def __getitem__(self,idx):\n","    item={key:torch.tensor(val[idx]) for key,val in self.encodings.items()}\n","    item['labels']=torch.tensor(self.labels[idx][:])\n","    return item\n","  \n","  def __len__(self):\n","    return len(self.labels)\n","\n","\n","\n","train_dataset=XFeiDataset(train_encoding,list(train_label))\n","val_dataset=XFeiDataset(val_encoding,list(val_label))\n","\n","# 单个读取到批量读取\n","from torch.utils.data import Dataset, DataLoader,TensorDataset\n","\n","train_loader=DataLoader(train_dataset,batch_size=batch_size,shuffle=True)\n","val_loader=DataLoader(val_dataset,batch_size=batch_size,shuffle=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1M7zS9z0LwOl"},"source":["for batch in train_dataset[:5]:\n","    print(batch)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t6ZEkJlnQxz3"},"source":["- 读取一个batch数据进行测试，输出input_ids、\n","  attention_mask、\n","  token_type_ids，形状都是torch.Size([3, 52])。\n","\n","- bert模型本身输出output有几部分：last_hidden_state和pooler_output及其它（待补充）。前者torch.Size([3,52,1024])，后者torch.Size([3,1024])。\n","\n","#也就是bert的最终输出，含有cls、sep、pad特殊字符部位的hidden向量，只是在后面计算loss时用mask'矩阵去掉。"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["4d8848f64a9640adb9b273b4ffca7efe","864078bec70f4dc5960cc135cf875771","3469de887d344cce9f2611251f6c69bf","8a4cbece899c44fbbb07a5f1cb9f6d2c","f6c62823af8b4cb98e6e903a9da4a8d6","fd093fcd0a24484e8ec08436a496e970","25750d755efb49f982e1dcea29cf3b40","e1c9ad732838455eb0a83ce5d4f95e30","8b665e34c7f64ac08478c588152fb158","1867b1b1a2fe4e1eadbb40998159f40c","5e82e33f12724c1d8b902d7f9431beb0"]},"id":"yb-qAdXv6-ao","executionInfo":{"status":"ok","timestamp":1636550032566,"user_tz":-480,"elapsed":49843,"user":{"displayName":"张hongxu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01344108933923387301"}},"outputId":"697b6d1e-0435-4659-9e6e-c44e9516845f"},"source":["from transformers import BertModel\n","model=BertModel.from_pretrained(config.roberta_model)\n","\n","for idx,batch in enumerate(train_loader):\n","  input_ids=batch['input_ids']\n","  attention_mask=batch['attention_mask']\n","  token_type_ids=batch['token_type_ids']\n","  #labels=batch['labels']\n","  output=model(input_ids,attention_mask,token_type_ids)\n","  print(input_ids)\n","  print(attention_mask)\n","  print(token_type_ids)\n","  print(output)\n","  print(output.last_hidden_state.shape,output.pooler_output.shape)\n","  break"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4d8848f64a9640adb9b273b4ffca7efe","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/1.22G [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at hfl/chinese-roberta-wwm-ext-large were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"]},{"output_type":"stream","name":"stdout","text":["tensor([[ 101, 3683, 1963, 1069,  689, 7213, 6121, 1050, 2940,  122,  121,  121,\n","         1039, 1506, 3418, 6809, 3172, 4851, 1171, 8024, 7444, 6206,  125,  130,\n","          121,  121,  121,  121, 4916, 1146, 8039,  769, 6858, 7213, 6121, 1050,\n","         2940,  122,  121,  121, 1039, 7716, 1377, 3797, 5384, 7481, 1259, 1171,\n","         8024,  102,    0,    0],\n","        [ 101,  812, 4638, 2141, 1213, 6852, 3933, 6375, 5307, 6814,  753, 2773,\n","         3819, 4851, 4638, 5401, 1744, 4374, 4277, 7607, 6121, 1447,  812, 6371,\n","         1377, 2400,  684, 2555, 2678, 8024,  800,  812, 2199, 2562, 2703, 1092,\n","         4958, 1092, 4638, 5318, 2190, 7566, 1818,  100,  100,  102,    0,    0,\n","            0,    0,    0,    0],\n","        [ 101, 3952, 2767,  704, 1908, 3325, 4638,  782, 7354, 1068, 5143,  680,\n","         2425, 1920, 1331, 7028, 4638,  686, 4518, 6225, 8024, 6963, 3291, 1872,\n","         3924,  517, 1184, 5296,  818, 1218,  518, 5143, 1154, 4638, 1079, 2159,\n","         3373, 3354,  705, 2168, 2428,  511,  102,    0,    0,    0,    0,    0,\n","            0,    0,    0,    0]])\n","tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n","         0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0]])\n","tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0],\n","        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0],\n","        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0]])\n","BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-1.4624e-01, -6.6930e-02, -6.3674e-02,  ..., -7.2607e-01,\n","          -2.4306e-01,  5.4834e-02],\n","         [ 4.3492e-01, -1.2708e+00, -1.8659e+00,  ...,  1.1655e-01,\n","          -4.6988e-01, -3.9702e-01],\n","         [ 7.9812e-01, -8.5796e-01, -1.9395e+00,  ..., -7.9845e-01,\n","           3.3927e-01, -2.2366e-01],\n","         ...,\n","         [-1.4626e-01, -6.6932e-02, -6.3647e-02,  ..., -7.2611e-01,\n","          -2.4302e-01,  5.4622e-02],\n","         [ 9.3144e-01,  5.8688e-01, -4.2193e-02,  ..., -7.2113e-01,\n","          -2.1883e-01, -3.2170e-01],\n","         [ 4.2660e-01,  5.6916e-01,  1.1420e-01,  ..., -2.3228e-01,\n","          -1.8053e-01,  3.5936e-01]],\n","\n","        [[-5.6661e-01,  5.8480e-02,  5.3021e-01,  ..., -8.6725e-01,\n","           3.3984e-02,  8.3469e-02],\n","         [-1.0431e+00,  8.0949e-01, -8.6252e-01,  ..., -1.2350e+00,\n","           4.7128e-03, -5.2444e-01],\n","         [ 5.1491e-01, -1.7887e-03,  1.5664e-01,  ...,  4.9653e-02,\n","           1.9738e-01, -5.7332e-01],\n","         ...,\n","         [ 2.8164e-02,  6.9006e-01, -4.2626e-01,  ..., -7.3319e-01,\n","          -9.1220e-02, -1.1055e-01],\n","         [-5.2030e-01,  4.4424e-01, -8.0008e-01,  ...,  1.2227e-01,\n","          -3.3460e-01,  6.5184e-01],\n","         [ 4.4505e-01,  3.7072e-01,  1.6097e-01,  ..., -3.1343e-01,\n","          -2.0498e-02, -4.8073e-01]],\n","\n","        [[ 1.8218e-01,  4.1021e-02,  2.2926e-01,  ..., -6.6636e-01,\n","           5.7848e-01, -3.7940e-01],\n","         [-5.9436e-01, -2.7598e-01, -8.7944e-01,  ..., -5.3915e-01,\n","           4.0974e-01,  4.0205e-01],\n","         [-3.8923e-01,  2.3530e-01, -6.0612e-02,  ..., -7.6360e-01,\n","           3.1026e-01, -1.1269e-01],\n","         ...,\n","         [ 5.6518e-01,  3.6801e-01, -4.0857e-01,  ..., -6.7604e-01,\n","          -8.0144e-02, -3.4079e-01],\n","         [ 4.3168e-01,  3.6269e-01, -3.9679e-01,  ..., -7.6165e-01,\n","           2.3792e-01, -4.7140e-01],\n","         [ 9.0256e-01,  5.1143e-01, -9.8757e-02,  ..., -5.7352e-01,\n","          -8.0810e-02, -5.5037e-01]]], grad_fn=<NativeLayerNormBackward>), pooler_output=tensor([[-0.6159, -0.0890, -0.4545,  ...,  0.7682, -0.6442,  0.4729],\n","        [-0.7383, -0.4546, -0.0673,  ...,  0.7485, -0.1857,  0.0142],\n","        [-0.3702, -0.2904,  0.3549,  ...,  0.6342, -0.6447,  0.2299]],\n","       grad_fn=<TanhBackward>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)\n","torch.Size([3, 52, 1024]) torch.Size([3, 1024])\n"]}]},{"cell_type":"markdown","metadata":{"id":"0Ypt8-ahUHqs"},"source":["- 切换模型为AutoModelForTokenClassification，输出结果只有logits，维度为torch.Size([3, 52, 31])\n","- 其实就是上个bert模型输出的last_hidden_state过一个nn.linear(1024,31)。如果后面接softmax就可以直接计算loss了。"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mLakkfx67sAz","executionInfo":{"status":"ok","timestamp":1636454902812,"user_tz":-480,"elapsed":2327,"user":{"displayName":"张hongxu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01344108933923387301"}},"outputId":"42a37b69-54e1-4d31-9641-45cdd664823b"},"source":["from transformers import AutoModelForTokenClassification\n","#model2=AutoModelForTokenClassification.from_pretrained(config.roberta_model,num_labels=31)\n","for idx,batch in enumerate(train_loader):\n","  input_ids=batch['input_ids']\n","  attention_mask=batch['attention_mask']\n","  token_type_ids=batch['token_type_ids']\n","  #labels=batch['labels']\n","  output=model2(input_ids,attention_mask,token_type_ids)\n","  print(output)\n","  print(output.logits.shape)\n","  break"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"]},{"output_type":"stream","name":"stdout","text":["TokenClassifierOutput(loss=None, logits=tensor([[[ 0.5434,  0.6106,  0.1762,  ..., -0.0830,  0.8065, -0.7689],\n","         [-0.0931, -0.1560,  1.4463,  ...,  0.2936,  0.6844, -0.2873],\n","         [ 0.9382, -0.2423,  0.7458,  ...,  0.9142,  0.5030, -0.5329],\n","         ...,\n","         [ 0.8264, -0.3213,  0.9057,  ...,  0.5539,  0.4696, -0.0120],\n","         [ 0.5696, -0.4459,  0.7491,  ...,  0.5282,  0.4338,  0.2392],\n","         [ 1.1784, -0.5461,  0.4984,  ...,  0.6217,  0.4835, -0.3212]],\n","\n","        [[ 0.3884,  0.5113,  0.2116,  ...,  0.1135,  0.7606, -0.7104],\n","         [ 0.9217, -0.7421,  0.6996,  ...,  0.5163, -0.0062, -0.1881],\n","         [ 0.1499, -0.6572,  1.2335,  ...,  0.1493, -0.0019,  0.2481],\n","         ...,\n","         [ 0.4536, -0.6493,  0.3247,  ...,  0.0290,  0.2193, -0.1074],\n","         [ 0.3182, -0.5207,  0.4987,  ..., -0.1983,  0.3622, -0.0031],\n","         [ 0.7298, -0.3522,  0.3433,  ...,  0.1259,  0.2976,  0.0196]],\n","\n","        [[ 0.5093,  0.4651,  0.1640,  ...,  0.0098,  0.7650, -0.6001],\n","         [-0.1943, -0.5667,  0.6609,  ...,  0.1457,  0.2120, -0.1712],\n","         [ 0.3929, -0.2277,  0.2582,  ...,  0.5201,  0.2465,  0.0311],\n","         ...,\n","         [ 0.3096, -0.6519,  0.7910,  ...,  0.2561,  0.1283,  0.1734],\n","         [ 0.7822, -0.7801,  0.6532,  ...,  0.3438,  0.3753,  0.0224],\n","         [ 0.7565, -0.7802,  0.6521,  ...,  0.2525,  0.3796,  0.0439]]],\n","       grad_fn=<AddBackward0>), hidden_states=None, attentions=None)\n","torch.Size([3, 52, 31])\n"]}]},{"cell_type":"code","metadata":{"id":"RcKd7JbCIu-b"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"89GJThxQI4KL"},"source":["##bert输出试验，固定编码信息\n","- 输入batch_size=3的数据,inputs_ids,attention_mask和token_type_ids都是torch.Size([3, 52])。\n","- bert模型和AutoModelForTokenClassification输出上面已经说过了"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LGpg2nLY83ze","executionInfo":{"status":"ok","timestamp":1636455214281,"user_tz":-480,"elapsed":353,"user":{"displayName":"张hongxu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01344108933923387301"}},"outputId":"42718c68-ec75-4353-f81c-b9de5d9c3e0c"},"source":["from torch import tensor\n","inputs_ids=tensor([[ 101, 3616, 5401, 1936, 2404,  160,  158,  149,  517, 4868, 4518,  123,\n","          518, 2135, 3175, 5381, 4991,  677, 5296,  102,    0,    0,    0,    0,\n","            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","            0,    0,    0,    0],\n","        [ 101,  123,  121,  121,  128, 2399,  809, 3341, 2692, 4508,  712, 7339,\n","          123, 5526,  122, 2398,  511, 1908, 6848,  124,  120,  122, 1377, 4937,\n","         7347,  511,  102,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","            0,    0,    0,    0],\n","        [ 101, 3797, 3857, 2272,  165,  143,  164,  147,  160,  157,  145,  153,\n","         3221, 6655, 4895, 4391, 3172, 5276,  124,  125,  121, 1062, 7027, 4638,\n","         1369,  671, 1905, 1936, 6225, 8024, 7770, 5276,  122,  126, 5101, 8024,\n","         7270,  122,  122,  121, 5101, 8024,  102,    0,    0,    0,    0,    0,\n","            0,    0,    0,    0]])\n","\n","attention_mask=tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0]])\n","\n","token_type_ids=tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0],\n","        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0],\n","        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0]])\n","print(inputs_ids.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([3, 52])\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b4eSYs78Sjnx","executionInfo":{"status":"ok","timestamp":1636404260902,"user_tz":-480,"elapsed":382,"user":{"displayName":"张hongxu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01344108933923387301"}},"outputId":"cde185b3-46f9-460d-fc7e-570f4089babc"},"source":["#wwm模型的特殊字符对应input_ids如下：\n","print(tokenizer.decode(101),tokenizer.decode(102),tokenizer.decode(0))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[CLS] [SEP] [PAD]\n"]}]},{"cell_type":"code","metadata":{"id":"NTuBjQkq8-9O"},"source":["output1=model(input_ids,attention_mask,token_type_ids)#bert模型，含last_hidden_state和pooler_output\n","output2=model2(input_ids,attention_mask,token_type_ids)#含token分类任务头的bert模型，输出只有logtis"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fHEAFxkaV9Zf"},"source":["##解读BertForTokenClassification任务头\n","1. 1.取bert输出的第一维，也就是句子中每个token的词向量，再dropout之后，经过nn.Linear得到logits。\n","2. 定义损失函数为loss_fct = CrossEntropyLoss()\n","3. 只保留loss的active parts\n","  - 3.1 计算时，将mask矩阵和logits都压缩。即B×L多个句子序列的token，压缩为一个超长序列的token\n","\n","  - 3.2 根据mask==1作为判断条件，根据torch.where方法取出labels中mask==1的值，mask==0的部分填充loss_fct.ignore_index，将labels赋值为active_labels。\n","\n","  - 3.3 计算logits和active_labels的loss。\n","\n","  - 3.4 没有mask矩阵是计算全部logits和labels的loss。\n","  \n",">测试了loss_fct.ignore_index=-100\n","\n",">torch.where中三者维度要一致。"]},{"cell_type":"code","metadata":{"id":"GpmPsOS8Lhvv"},"source":["#这里就以token任务头举例子。logits是last_hidden_state经过一个linear得到\n","def __init__(self, config):\n","  super().__init__(config)\n","  self.num_labels = config.num_labels\n","\n","  self.bert = BertModel(config, add_pooling_layer=False)#加载模型\n","  classifier_dropout = (config.classifier_dropout)\n","  self.dropout = nn.Dropout(classifier_dropout)#定义dropout\n","  self.classifier = nn.Linear(config.hidden_size, config.num_labels)#定义nn.Linear\n","\n","  self.init_weights()\n","  outputs = self.bert(\n","            input_ids,\n","            attention_mask=attention_mask,\n","            token_type_ids=token_type_ids,\n","            position_ids=position_ids,\n","            head_mask=head_mask,\n","            inputs_embeds=inputs_embeds,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states,\n","            return_dict=return_dict)\n","  \n","#1.取bert输出的第一维，也就是句子中每个token的词向量，再dropout之后，经过nn.Linear得到logits。  \n","sequence_output=outputs[0]   \n","sequence_output=self.dropout(sequence_output)#\n","logits=self.classifier(sequence_output)\n","\n","loss = None\n","if labels is not None:\n","  #2.定义损失函数\n","  loss_fct = CrossEntropyLoss()\n","\n","  #3.只保留loss的active parts\n","  #3.1计算时，将mask矩阵和logits都压缩。即B×L多个句子序列的token，压缩为一个超长序列的token\n","  #3.2根据mask==1作为判断条件，根据torch.where方法取出labels中mask==1的值，mask==0的部分填充loss_fct.ignore_index，将labels赋值为active_labels。测试了loss_fct.ignore_index=-100.\n","  #3.3计算logits和active_labels的loss。\n","  #3.4没有mask矩阵是计算全部logits和labels的loss。\n","  if attention_mask is not None:\n","    active_loss=attention_mask.view(-1)==1#判断attention_mask=1的真假mask矩阵，也是一维\n","    active_logits=logits.view(-1,self.num_labels)#logits变成一维\n","    active_labels=torch.where(\n","        active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels))\n","    ##torch.tensor(loss_fct.ignore_index).type_as(labels)就是一个全部为loss忽视索引，形状和labels一样的矩阵。\n","    loss=loss_fct(active_logits,active_labels)\n","  else:\n","    loss=loss_fct(logits.view(-1,self.num_labels),labels.view(-1))\n","\n","if not return_dict:\n","  output=(logits,)+outputs[2:]\n","  return((loss,)+ output) if loss is not None else output\n","\n","return TokenClassifierOutput(\n","    loss=loss,\n","    logits=logits,\n","    hidden_states=outputs.hidden_states,\n","    attentions=outputs.attentions,)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yf0ux4z6V7vk"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IL_YBOXDZHvh"},"source":["##测试view效果。\n","- input_ids，mask矩阵等原本torch.Size([3,52])，变成torch.Size([156])\n","- logits.view(-1,num_labels)由torch.Size([3,52，1024])变成torch.Size([156, 1024])\n","\n","- active_loss=attention_mask.view(-1)==1,变成一个一维True，Flase矩阵，用于torch.where的判断\n","\n","#总之token标注等分类，就是把一个batch_size的多个序列（句子）的token变成一个超长序列，只计算attention=1的部位的loss。也就是多句成一句来token分类。"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NbJI3wthE-b8","executionInfo":{"status":"ok","timestamp":1636456666566,"user_tz":-480,"elapsed":369,"user":{"displayName":"张hongxu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01344108933923387301"}},"outputId":"f1df14dc-cf6c-4e57-ec17-4d7f7bdd788d"},"source":["active_logits=output2.logits.view(-1,31)\n","print(active_logits,active_logits.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 0.6373,  0.4533,  0.2777,  ..., -0.0721,  0.7208, -0.5921],\n","        [-0.1141, -0.1505,  1.6466,  ...,  0.5542,  0.6734, -0.1642],\n","        [ 0.7736, -0.2756,  0.8982,  ...,  0.9006,  0.2669, -0.3836],\n","        ...,\n","        [ 0.5322, -0.6047,  0.8029,  ...,  0.2935,  0.5240,  0.1276],\n","        [ 0.6182, -0.6449,  0.7590,  ...,  0.2894,  0.5287,  0.1530],\n","        [ 0.5741, -0.7172,  0.6675,  ...,  0.1376,  0.6363,  0.0436]],\n","       grad_fn=<ViewBackward>) torch.Size([156, 31])\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PPsKVuv0LFg9","executionInfo":{"status":"ok","timestamp":1636456709722,"user_tz":-480,"elapsed":361,"user":{"displayName":"张hongxu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01344108933923387301"}},"outputId":"3f1aeba6-7c17-45d2-b135-7b6565296579"},"source":["#压缩并计算mask矩阵。原本torch.Size([3,52])变成torch.Size([156])\n","active_loss=attention_mask.view(-1)\n","print(active_loss,active_loss.shape)\n","active_loss=attention_mask.view(-1)==1\n","active_loss"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]) torch.Size([156])\n"]},{"output_type":"execute_result","data":{"text/plain":["tensor([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n","         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n","        False, False, False, False, False, False, False, False, False, False,\n","        False, False, False, False, False, False, False, False, False, False,\n","        False, False, False, False, False, False, False, False, False, False,\n","        False, False,  True,  True,  True,  True,  True,  True,  True,  True,\n","         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n","         True,  True,  True,  True,  True,  True,  True,  True,  True, False,\n","        False, False, False, False, False, False, False, False, False, False,\n","        False, False, False, False, False, False, False, False, False, False,\n","        False, False, False, False,  True,  True,  True,  True,  True,  True,\n","         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n","         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n","         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n","         True,  True,  True,  True,  True,  True,  True, False, False, False,\n","        False, False, False, False, False, False])"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","metadata":{"id":"WNpntDvUOW6N"},"source":["result=torch.where(condition>0.5,x,y)\n","#x：若满足条件，则取x中元素\n","#y：若不满足条件，则取y中元素"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dGZf7VfCQVFy"},"source":["##综上,计算loss时忽略padding部分（attention_mask==1）步骤为：\n","- 先取出mask矩阵压缩为一维，==1设为真假矩阵。\n","- labels压缩，再创建一个同形状的loss ignore_index矩阵\n","- torch.where取出labels对应mask==1的部分，其余部分为loss忽略索引。即labels为（mask==1和忽略部分）\n","- 计算logtis和active_labels的loss。\n","\n","然而，只有pad部分attention_mask==0,句子首尾部分的cls和sep还是计算loss的。本身这样处理还留有cls和sep。\n","\n","## labels和word_ids\n","之前处理labels的时候，根据word_ids（特殊字符为none），将labels设置为input_ids一样的长度。特殊字符处labels添加为-100.所以可以把mask句子替换为word_ids矩阵解决问题。\n"]},{"cell_type":"code","metadata":{"id":"-CQbcVeJVnss"},"source":["def tokenize_and_align_labels(examples):\n","  tokenized_inputs=tokenizer(examples[\"tokens\"],truncation=True,is_split_into_words=True)#数据分词\n","  labels = []#创建labels列表\n","  for i,label in enumerate(examples[\"labels\"]):\n","    word_ids=tokenized_inputs.word_ids(batch_index=i)#取出索引i的编码数据的word_ids属性\n","    aligned_labels=[-100 if i is None else example[\"labels\"][i] for i in word_ids]    \n","    labels.append(aligned_labels)\n","\n","  tokenized_inputs[\"labels\"]=labels\n","  return tokenized_inputs\n","\n","loss=loss_fct(active_logits,tokenized_inputs[\"labels\"])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fb86deg8Z-1o","executionInfo":{"status":"ok","timestamp":1636406634999,"user_tz":-480,"elapsed":383,"user":{"displayName":"张hongxu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01344108933923387301"}},"outputId":"18e259a0-9488-44db-8fa8-c4aca43f3ba9"},"source":["loss_fct=torch.nn.NLLLoss()\n","ignore=loss_fct.ignore_index\n","\n","print(ignore)\n","#可见将特殊字符的labels设置为-100就可以了。计算loss时直接忽略。但是这只是直接用bert的时候。"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["-100\n"]}]},{"cell_type":"code","metadata":{"id":"wlLgGg97crsc"},"source":["方法一;考虑将logtis根据labels进行处理，将所有为-100的位置设置为pad。\n","方法二：根据attention mask去除padding部分，再掐头去尾扔掉cls和sep。得到完整的ast_hidden_state。pad之后输入LSTM。"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LFe1R8aMcCL7"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"putqMIZgcNDm"},"source":["##测试lstm的输出，和整理函数、pad打包解包函数。（实验在本地jyputer写的，因为colab装不了nltk工具包）结果发在csdn[《lstm token分类模型代码解析》](https://blog.csdn.net/qq_56591814/article/details/121237751)"]},{"cell_type":"code","metadata":{"id":"KoW_IMn_cLOc"},"source":["from torch import tensor\n","import torch\n","from torch import nn, optim\n","from torch.nn import functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n","#设置一个句子长度2,4,7的tensor。即batch_size=3.\n","\n","\n","#在整理函数里面pad。\n","def collate_fn(examples):\n","  padding_value=0\n","  lengths = torch.tensor([len(ex[0]) for ex in examples])#得到一个batch的句子长度\n","  inputs = [torch.tensor(ex[0]) for ex in examples]#取出batch的句子向量\n","  targets = [torch.tensor(ex[1]) for ex in examples]\n","  #将变长输入和标签都pad到同一长度\n","  inputs = pad_sequence(inputs,batch_first=True,padding_value=padding_value)\n","  targets = pad_sequence(targets,batch_first=True,padding_value=padding_value)\n","  return inputs,lengths,targets,inputs!=padding_value\n","\n"],"execution_count":null,"outputs":[]}]}